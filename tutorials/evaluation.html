
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>LLM Evaluation &#8212; AdalFlow: The Library to Build and Auto-Optimize LLM Task Pipelines</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=af51538a" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/css/rtd_sphinx_search.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../_static/js/rtd_search_config.js"></script>
    <script src="../_static/js/rtd_sphinx_search.min.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'tutorials/evaluation';</script>
    <link rel="icon" href="../_static/LightRAG-logo-circle.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Datasets" href="datasets.html" />
    <link rel="prev" title="Agent" href="agent.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/adalflow-logo.png" class="logo__image only-light" alt="AdalFlow: The Library to Build and Auto-Optimize LLM Task Pipelines - Home"/>
    <script>document.write(`<img src="../_static/adalflow-logo.png" class="logo__image only-dark" alt="AdalFlow: The Library to Build and Auto-Optimize LLM Task Pipelines - Home"/>`);</script>
  
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../get_started/index.html">
    Get Started
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="index.html">
    Tutorials
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../use_cases/index.html">
    Use Cases
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../contributor/index.html">
    Contributor Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../apis/index.html">
    API Reference
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
        </div>
      
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/SylphAI-Inc/AdalFlow" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discord.gg/ezzszrRZvT" title="Discord" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discord fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discord</span></a>
        </li>
</ul></div>
      
        <div class="navbar-item">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../get_started/index.html">
    Get Started
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="index.html">
    Tutorials
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../use_cases/index.html">
    Use Cases
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../contributor/index.html">
    Contributor Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../apis/index.html">
    API Reference
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/SylphAI-Inc/AdalFlow" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discord.gg/ezzszrRZvT" title="Discord" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discord fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discord</span></a>
        </li>
</ul></div>
        
          <div class="navbar-item">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lightrag_design_philosophy.html">Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="class_hierarchy.html">Class Hierarchy</a></li>
<li class="toctree-l1"><a class="reference internal" href="trace_graph.html">AdalFlow Trace Graph</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Base Classes</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="component.html">Component</a></li>
<li class="toctree-l1"><a class="reference internal" href="base_data_class.html">DataClass</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">RAG Essentials</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="prompt.html">Prompt</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_client.html">ModelClient</a></li>
<li class="toctree-l1"><a class="reference internal" href="generator.html">Generator</a></li>

<li class="toctree-l1"><a class="reference internal" href="output_parsers.html">Parser</a></li>
<li class="toctree-l1"><a class="reference internal" href="embedder.html">Embedder</a></li>
<li class="toctree-l1"><a class="reference internal" href="retriever.html">Retriever</a></li>
<li class="toctree-l1"><a class="reference internal" href="text_splitter.html">Text Splitter</a></li>
<li class="toctree-l1"><a class="reference internal" href="db.html">Data (Database/Pipeline)</a></li>
<li class="toctree-l1"><a class="reference internal" href="rag_playbook.html">RAG Playbook</a></li>
<li class="toctree-l1"><a class="reference internal" href="rag_with_memory.html">RAG with Memory</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Agent Essentials</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="tool_helper.html">Function calls</a></li>
<li class="toctree-l1"><a class="reference internal" href="agent.html">Agent</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Evaluating</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">LLM Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="datasets.html">Datasets</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Logging &amp; Tracing</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="logging.html">Logger Example</a></li>


<li class="toctree-l1"><a class="reference internal" href="logging_tracing.html">Tracing</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="index.html" class="nav-link">Tutorials</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">LLM Evaluation</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div style="display: flex; justify-content: flex-start; align-items: center; margin-bottom: 20px;">

     <a href="https://colab.research.google.com/github/SylphAI-Inc/AdalFlow/blob/main/notebooks/evaluation/adalflow_llm_eval.ipynb" target="_blank" style="margin-right: 10px;">
         <img alt="Try Quickstart in Colab" src="https://colab.research.google.com/assets/colab-badge.svg" style="vertical-align: middle;">
     </a>
   <a href="https://github.com/SylphAI-Inc/AdalFlow/blob/main/tutorials/evaluation" target="_blank" style="display: flex; align-items: center;">
      <img src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png" alt="GitHub" style="height: 20px; width: 20px; margin-right: 5px;">
      <span style="vertical-align: middle;"> Open Source Code</span>
   </a>
</div><section id="llm-evaluation">
<h1>LLM Evaluation<a class="headerlink" href="#llm-evaluation" title="Link to this heading">#</a></h1>
<p>“You cannot optimize what you cannot measure”.</p>
<p>This is especially true in the context of LLMs, which have become increasingly popular due to their impressive performance on a wide range of tasks.
Evaluating LLMs and their applications is crucial in both research and production to understand their capabilities and limitations.
Overall, such evaluation is a complex and multifaceted process.
Below, we provide a guideline for evaluating LLMs and their applications, incorporating aspects outlined by <em>Chang et al.</em> <a class="footnote-reference brackets" href="#id39" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> and more for RAG evaluation.</p>
<ul class="simple">
<li><p><strong>What to evaluate</strong>: the tasks and capabilities that LLMs are evaluated on.</p></li>
<li><p><strong>Where to evaluate</strong>: the datasets and benchmarks that are used for evaluation.</p></li>
<li><p><strong>How to evaluate</strong>: the protocols and metrics that are used for evaluation.</p></li>
</ul>
<section id="tasks-and-capabilities">
<h2>Tasks and Capabilities<a class="headerlink" href="#tasks-and-capabilities" title="Link to this heading">#</a></h2>
<p>Below are some commonly evaluated tasks and capabilities of LLMs summarized in <a class="footnote-reference brackets" href="#id39" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>.</p>
<ul class="simple">
<li><p><em>Natural language understanding</em> (NLU) tasks, such as text classification and sentiment analysis, which evaluate the LLM’s ability to understand natural language.</p></li>
<li><p><em>Natural language generation</em> (NLG) tasks, such as text summarization, translation, and question answering, which evaluate the LLM’s ability to generate natural language.</p></li>
<li><p><em>Reasoning</em> tasks, such as mathematical, logic, and common-sense reasoning, which evaluate the LLM’s ability to perform reasoning and inference to obtain the correct answer.</p></li>
<li><p><em>Robustness</em>, which evaluate the LLM’s ability to generalize to unexpected inputs.</p></li>
<li><p><em>Fairness</em>, which evaluate the LLM’s ability to make unbiased decisions.</p></li>
<li><p><em>Domain adaptation</em>, which evaluate the LLM’s ability to adapt from general language to specific new domains, such as medical or legal texts, coding, etc.</p></li>
<li><p><em>Agent applications</em>, which evaluate the LLM’s ability to use external tools and APIs to perform tasks, such as web search.</p></li>
</ul>
<p>For a more detailed and comprehensive description of the tasks and capabilities that LLMs are evaluated on, please refer to the review papers by <em>Chang et al.</em> <a class="footnote-reference brackets" href="#id39" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> and <em>Guo et al.</em> <a class="footnote-reference brackets" href="#id40" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>.
RRAG <a class="footnote-reference brackets" href="#id59" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>21<span class="fn-bracket">]</span></a> evaluation differs as it introduces a retrieval component to the pipeline, which we will discuss in the next section.</p>
</section>
<section id="datasets-and-benchmarks">
<h2>Datasets and Benchmarks<a class="headerlink" href="#datasets-and-benchmarks" title="Link to this heading">#</a></h2>
<p>The selection of datasets and benchmarks <a class="footnote-reference brackets" href="#id57" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>19<span class="fn-bracket">]</span></a> is important, as it determines the quality and relevance of the evaluation.</p>
<p>To comprehensively assess the capabilities of LLMs, researchers typically utilize benchmarks and datasets that span a broad spectrum of tasks. For example, in the GPT-4 technical report <a class="footnote-reference brackets" href="#id41" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>, the authors employed a variety of general language benchmarks, such as MMLU <a class="footnote-reference brackets" href="#id42" id="id8" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a>, and academic exams, such as the SAT, GRE, and AP courses, to evaluate the diverse capabilities of GPT-4. Below are some commonly used datasets and benchmarks for evaluating LLMs.</p>
<ul class="simple">
<li><p><em>MMLU</em> <a class="footnote-reference brackets" href="#id42" id="id9" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a>, which evaluates the LLM’s ability to perform a wide range of language understanding tasks.</p></li>
<li><p><em>HumanEval</em> <a class="footnote-reference brackets" href="#id43" id="id10" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a>, which measures the LLM’s capability in writing Python code.</p></li>
<li><p><a class="reference external" href="https://crfm.stanford.edu/helm/">HELM</a>, which evaluates LLMs across diverse aspects such as language understanding, generation, common-sense reasoning, and domain adaptation.</p></li>
<li><p><a class="reference external" href="https://arena.lmsys.org/">Chatbot Arena</a>, which is an open platform to evaluate LLMs through human voting.</p></li>
<li><p><a class="reference external" href="https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/api-bank">API-Bank</a>, which evaluates LLMs’ ability to use external tools and APIs to perform tasks.</p></li>
</ul>
<p>Please refer to the review papers (<em>Chang et al.</em> <a class="footnote-reference brackets" href="#id39" id="id11" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>, <em>Guo et al.</em> <a class="footnote-reference brackets" href="#id40" id="id12" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>, and <em>Liu et al.</em> <a class="footnote-reference brackets" href="#id44" id="id13" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a>) for a more comprehensive overview of the datasets and benchmarks used in LLM evaluations. Additionally, a lot of datasets are readily accessible via the <a class="reference external" href="https://huggingface.co/datasets">Hugging Face Datasets</a> library. For instance, the MMLU dataset can be easily loaded from the Hub using the following code snippet.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="linenos">2</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="s2">&quot;cais/mmlu&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;abstract_algebra&#39;</span><span class="p">)</span>
<span class="linenos">3</span><span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">])</span>
</pre></div>
</div>
<p>The output will be a Dataset object containing the test set of the MMLU dataset.</p>
<p><strong>Datasets for RAG Evaluation</strong></p>
<p>According to RAGEval <a class="footnote-reference brackets" href="#id59" id="id14" role="doc-noteref"><span class="fn-bracket">[</span>21<span class="fn-bracket">]</span></a>, the evaluation dataset can be categorized into two types: traditional open-domain QA datasets and scenario-specific RAG evaluation datasets.</p>
<p>Traditional open-domain QA datasets include:</p>
<ul class="simple">
<li><p>HotPotQA: A dataset for multi-hop question answering.</p></li>
<li><p>Natural Questions: A dataset for open-domain question answering.</p></li>
<li><p>MS MARCO: A dataset for passage retrieval and question answering.</p></li>
<li><p>2WikiMultiHopQA: A dataset for multi-hop question answering.</p></li>
<li><p>KILT: A benchmark for knowledge-intensive language tasks.</p></li>
</ul>
<p>Scenario-specific RAG evaluation datasets,</p>
<ul class="simple">
<li><p>RGB: assesses LLMs’ ability to lever-age retrieved information, focusing on noise ro-bustness and information integration.</p></li>
<li><p>CRAG: increases domain coverage and introducesmock APIs to simulate real-world retrieval sce-narios.</p></li>
</ul>
</section>
<section id="evaluation-metrics">
<h2>Evaluation Metrics<a class="headerlink" href="#evaluation-metrics" title="Link to this heading">#</a></h2>
<p>Evaluation methods can be divided into <em>automated evaluation</em> and <em>human evaluation</em> (<em>Chang et al.</em> <a class="footnote-reference brackets" href="#id39" id="id15" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> and <em>Liu et al.</em> <a class="footnote-reference brackets" href="#id44" id="id16" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a>).</p>
<p>Automated evaluation typically involves using metrics such as accuracy and BERTScore or employing an LLM as the judge, to quantitatively assess the performance of LLMs on specific tasks.
Human evaluation, on the other hand, involves human in the loop to evaluate the quality of the generated text or the performance of the LLM.</p>
<p>Here, we categorize the automated evaluation methods as follows:</p>
<ol class="arabic simple">
<li><p>For classicial NLU tasks, such as text classification and sentiment analysis, you can use metrics such as accuracy, F1-score, and ROC-AUC to evaluate the performance of LLM response just like you would do using non-genAI models. You can check out <a class="reference external" href="https://lightning.ai/docs/torchmetrics">TorchMetrics</a>.</p></li>
<li><p>For NLG tasks, such as text summarization, translation, and question answering: (1) you can use metrics such as ROUGE, BLEU, METEOR, and BERTScore, perplexity, <a class="reference internal" href="../apis/eval/eval.llm_as_judge.html#module-eval.llm_as_judge" title="eval.llm_as_judge"><code class="xref py py-class docutils literal notranslate"><span class="pre">LLMasJudge</span></code></a> etc to evaluate the quality of the generated text with respect to the reference text.
Or using <a class="reference internal" href="../apis/eval/eval.g_eval.html#module-eval.g_eval" title="eval.g_eval"><code class="xref py py-class docutils literal notranslate"><span class="pre">GEvalLLMJudge</span></code></a> to evaluate the generated text even without reference text.</p></li>
<li><p>For RAG (Retrieval-Augmented Generation) pipelines, you can use metrics such as <a class="reference internal" href="../apis/eval/eval.retriever_recall.html#module-eval.retriever_recall" title="eval.retriever_recall"><code class="xref py py-class docutils literal notranslate"><span class="pre">RetrieverRecall</span></code></a>, <a class="reference internal" href="../apis/eval/eval.answer_match_acc.html#module-eval.answer_match_acc" title="eval.answer_match_acc"><code class="xref py py-class docutils literal notranslate"><span class="pre">AnswerMatchAcc</span></code></a>, and <a class="reference internal" href="../apis/eval/eval.llm_as_judge.html#module-eval.llm_as_judge" title="eval.llm_as_judge"><code class="xref py py-class docutils literal notranslate"><span class="pre">LLMasJudge</span></code></a> to evaluate the quality of the retrieved context and the generated answer.</p></li>
</ol>
<p>You can also check out the metrics provided by <a class="reference external" href="https://huggingface.co/metrics">Hugging Face Metrics</a>, <a class="reference external" href="https://docs.ragas.io/en/stable/getstarted/index.html">RAGAS</a>,  <a class="reference external" href="https://lightning.ai/docs/torchmetrics/stable/">TorchMetrics</a>, <a class="reference external" href="https://arxiv.org/abs/2311.09476">ARES</a>, <a class="reference external" href="https://arxiv.org/abs/2401.17072">SemScore</a>, <a class="reference external" href="https://ojs.aaai.org/index.php/AAAI/article/view/29728">RGB</a>, etc.</p>
</section>
<section id="nlg-evaluation">
<h2>NLG Evaluation<a class="headerlink" href="#nlg-evaluation" title="Link to this heading">#</a></h2>
<section id="classicial-string-metrics">
<h3>Classicial String Metrics<a class="headerlink" href="#classicial-string-metrics" title="Link to this heading">#</a></h3>
<p>The simplest metric would be EM <a class="reference internal" href="../apis/eval/eval.answer_match_acc.html#module-eval.answer_match_acc" title="eval.answer_match_acc"><code class="xref py py-class docutils literal notranslate"><span class="pre">AnswerMatchAcc</span></code></a>: This calculates the exact match accuracy or fuzzy match accuracy of the generated answers by comparing them to the ground truth answers.</p>
<p>More advanced traditional metrics, such as F1, BLEU <a class="footnote-reference brackets" href="#id46" id="id18" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a>, ROUGE <a class="footnote-reference brackets" href="#id47" id="id19" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a>, <a class="footnote-reference brackets" href="#id58" id="id20" role="doc-noteref"><span class="fn-bracket">[</span>20<span class="fn-bracket">]</span></a>, and METEOR <a class="footnote-reference brackets" href="#id50" id="id21" role="doc-noteref"><span class="fn-bracket">[</span>12<span class="fn-bracket">]</span></a>, may fail to capture the semantic similarity between the reference text and the generated text, resulting in low correlation with human judgment.</p>
<p>You can use <cite>TorchMetrics</cite> <a class="footnote-reference brackets" href="#id48" id="id22" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a> or <a class="reference external" href="https://huggingface.co/metrics">Hugging Face Metrics</a> to compute these metrics. For instance,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">gt</span> <span class="o">=</span> <span class="s2">&quot;Brazil has won 5 FIFA World Cup titles&quot;</span>
<span class="n">pred</span> <span class="o">=</span> <span class="s2">&quot;Brazil is the five-time champion of the FIFA WorldCup.&quot;</span>

<span class="k">def</span> <span class="nf">compute_rouge</span><span class="p">(</span><span class="n">gt</span><span class="p">,</span> <span class="n">pred</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">torchmetrics.text.rouge</span> <span class="kn">import</span> <span class="n">ROUGEScore</span>

    <span class="n">rouge</span> <span class="o">=</span> <span class="n">ROUGEScore</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">rouge</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">gt</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">compute_bleu</span><span class="p">(</span><span class="n">gt</span><span class="p">,</span> <span class="n">pred</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">torchmetrics.text.bleu</span> <span class="kn">import</span> <span class="n">BLEUScore</span>

    <span class="n">bleu</span> <span class="o">=</span> <span class="n">BLEUScore</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">bleu</span><span class="p">([</span><span class="n">pred</span><span class="p">],</span> <span class="p">[[</span><span class="n">gt</span><span class="p">]])</span>
</pre></div>
</div>
<p>The output Rouge score is:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="err">&#39;rouge</span><span class="mi">1</span><span class="err">_</span><span class="kc">f</span><span class="err">measure&#39;</span><span class="p">:</span><span class="w"> </span><span class="kc">tens</span><span class="err">or(</span><span class="mf">0.2222</span><span class="err">)</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;rouge</span><span class="mi">1</span><span class="err">_precisio</span><span class="kc">n</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="kc">tens</span><span class="err">or(</span><span class="mf">0.2000</span><span class="err">)</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;rouge</span><span class="mi">1</span><span class="err">_recall&#39;</span><span class="p">:</span><span class="w"> </span><span class="kc">tens</span><span class="err">or(</span><span class="mf">0.2500</span><span class="err">)</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;rouge</span><span class="mi">2</span><span class="err">_</span><span class="kc">f</span><span class="err">measure&#39;</span><span class="p">:</span><span class="w"> </span><span class="kc">tens</span><span class="err">or(</span><span class="mf">0.</span><span class="err">)</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;rouge</span><span class="mi">2</span><span class="err">_precisio</span><span class="kc">n</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="kc">tens</span><span class="err">or(</span><span class="mf">0.</span><span class="err">)</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;rouge</span><span class="mi">2</span><span class="err">_recall&#39;</span><span class="p">:</span><span class="w"> </span><span class="kc">tens</span><span class="err">or(</span><span class="mf">0.</span><span class="err">)</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;rougeL_</span><span class="kc">f</span><span class="err">measure&#39;</span><span class="p">:</span><span class="w"> </span><span class="kc">tens</span><span class="err">or(</span><span class="mf">0.2222</span><span class="err">)</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;rougeL_precisio</span><span class="kc">n</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="kc">tens</span><span class="err">or(</span><span class="mf">0.2000</span><span class="err">)</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;rougeL_recall&#39;</span><span class="p">:</span><span class="w"> </span><span class="kc">tens</span><span class="err">or(</span><span class="mf">0.2500</span><span class="err">)</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;rougeLsum_</span><span class="kc">f</span><span class="err">measure&#39;</span><span class="p">:</span><span class="w"> </span><span class="kc">tens</span><span class="err">or(</span><span class="mf">0.2222</span><span class="err">)</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;rougeLsum_precisio</span><span class="kc">n</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="kc">tens</span><span class="err">or(</span><span class="mf">0.2000</span><span class="err">)</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;rougeLsum_recall&#39;</span><span class="p">:</span><span class="w"> </span><span class="kc">tens</span><span class="err">or(</span><span class="mf">0.2500</span><span class="err">)</span><span class="p">}</span>
</pre></div>
</div>
<p>The output BLEU score is: 0.0</p>
<p>These two sentences totally mean the same, but it scored low in BLEU and ROUGE.</p>
</section>
<section id="embedding-based-metrics">
<h3>Embedding-based Metrics<a class="headerlink" href="#embedding-based-metrics" title="Link to this heading">#</a></h3>
<p>To make up for this, embedding-based  metrics or neural evaluators such as BERTScore was created.
You can find BERTScore in both <a class="reference external" href="https://huggingface.co/metrics">Hugging Face Metrics</a> and <a class="reference external" href="https://lightning.ai/docs/torchmetrics/stable/text/bertscore.html">TorchMetrics</a>.
BERTScore uses pre-trained contextual embeddings from BERT and matched words in generated text and reference text using cosine similarity.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_bertscore</span><span class="p">(</span><span class="n">gt</span><span class="p">,</span> <span class="n">pred</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    https://lightning.ai/docs/torchmetrics/stable/text/bert_score.html</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">torchmetrics.text.bert</span> <span class="kn">import</span> <span class="n">BERTScore</span>

    <span class="n">bertscore</span> <span class="o">=</span> <span class="n">BERTScore</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">bertscore</span><span class="p">([</span><span class="n">pred</span><span class="p">],</span> <span class="p">[</span><span class="n">gt</span><span class="p">])</span>
</pre></div>
</div>
<p>The output BERT score is:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="err">&#39;precisio</span><span class="kc">n</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="kc">tens</span><span class="err">or(</span><span class="mf">0.9752</span><span class="err">)</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;recall&#39;</span><span class="p">:</span><span class="w"> </span><span class="kc">tens</span><span class="err">or(</span><span class="mf">0.9827</span><span class="err">)</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="kc">f</span><span class="mi">1</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="kc">tens</span><span class="err">or(</span><span class="mf">0.9789</span><span class="err">)</span><span class="p">}</span>
</pre></div>
</div>
<p>This score does reflect the semantic similarity between the two sentences almost perfectly.
However, the downside of all the above metrics is that you need to have a reference text to compare with.
Labeling, such as creating a reference text, can be quite challenging in many NLG tasks, such as summarization.</p>
</section>
<section id="llm-as-judge">
<h3>LLM as Judge<a class="headerlink" href="#llm-as-judge" title="Link to this heading">#</a></h3>
<blockquote>
<div><p>Just as how LLM has made the AI tasks easier, it has made the evaluation of AI tasks easier too.</p>
</div></blockquote>
<p>The real power of using LLM as judge is:</p>
<ol class="arabic simple">
<li><p>its <strong>adaptatibility</strong> compared with all the above metrics, it can be adapted to any task out of the box.</p></li>
<li><p>its <strong>flexibility</strong> and <strong>robustness</strong> at measuring. For many NLG tasks, there can only have multiple references or even countless correct reponses. Using traditional metrics can be very limiting.</p></li>
<li><p><strong>Less training data</strong>. Align an LLM judge to your task using (question, ground truth, generated text, gt_score) tuples takes less data than finetune a model like BERTScore.</p></li>
</ol>
<p>Evaluating an LLM application using an LLM as a judge is similar to building an LLM task pipeline.
Developers need to understand the underlying prompt used by the LLM judge to determine whether the default judge is sufficient or if customization is required.</p>
<p>After reviewing research papers and existing libraries, we found no solution that provides these evaluators with complete clarity without requiring developers to install numerous additional dependencies.
With this in mind, AdalFlow decided to offer a comprehensive set of LLM evaluators rather than directing our developers to external evaluation packages.</p>
<p>You can use an LLM as a judge in cases where you have a reference text or not.
The key is to clearly define the metric using text.</p>
<p><strong>We are developing LLM judge to replace human labelers, boosting efficiency and reducing financial costs.</strong></p>
<p>The most straightforward LLM judge predicts a yes/no answer or a float score in range [0, 1] based on the comparison between the generated text and the reference text for a given judgment query.</p>
<p>Here is AdalFlow’s default judegement query:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">DEFAULT_JUDGEMENT_QUERY</span> <span class="o">=</span> <span class="s2">&quot;Does the predicted answer contain the ground truth answer? Say True if yes, False if no.&quot;</span>
</pre></div>
</div>
<p>AdalFlow provides a very customizable LLM judge, which can be used in three ways:</p>
<ol class="arabic simple">
<li><p>With question, ground truth, and generated text</p></li>
<li><p>Without question, with ground truth, and generated text, mainly matching the ground truth and the generated text</p></li>
<li><p>With question, without ground truth, with generated text, mainly matching between the questiona and the generated text</p></li>
</ol>
<p>And you can customize the <cite>judgement_query</cite> towards your use case or even the whole llm template.</p>
<p>AdalFlow LLM judge returns <cite>LLMJudgeEvalResult</cite> which has three fields:
1. <cite>avg_score</cite>: average score of the generated text
2. <cite>judgement_score_list</cite>: list of scores for each generated text
3. <cite>confidence_interval</cite>: a tuple of the 95% confidence interval of the scores</p>
<p><cite>DefaultLLMJudge</cite> is an LLM task pipeline that takes a single question(optional), ground truth(optional), and generated text and returns the float score in range [0,1].</p>
<p>You can use it as an <cite>eval_fn</cite> for AdalFlow Trainer.</p>
<p><cite>LLMAsJudge</cite> is an evaluator that takes a list of inputs and returns a list of <cite>LLMJudgeEvalResult</cite>.
Besides of the score, it computes the confidence interval of the scores.</p>
<p><strong>Case 1: With References</strong></p>
<p>Now, you can use the following code to calculate the final score based on the judgment query:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_llm_as_judge</span><span class="p">():</span>
    <span class="kn">import</span> <span class="nn">adalflow</span> <span class="k">as</span> <span class="nn">adal</span>
    <span class="kn">from</span> <span class="nn">adalflow.eval.llm_as_judge</span> <span class="kn">import</span> <span class="n">LLMasJudge</span><span class="p">,</span> <span class="n">DefaultLLMJudge</span>
    <span class="kn">from</span> <span class="nn">adalflow.components.model_client</span> <span class="kn">import</span> <span class="n">OpenAIClient</span>

    <span class="n">adal</span><span class="o">.</span><span class="n">setup_env</span><span class="p">()</span>

    <span class="n">questions</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;Is Beijing in China?&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Is Apple founded before Google?&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Is earth flat?&quot;</span><span class="p">,</span>
    <span class="p">]</span>
    <span class="n">pred_answers</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Yes&quot;</span><span class="p">,</span> <span class="s2">&quot;Yes, Appled is founded before Google&quot;</span><span class="p">,</span> <span class="s2">&quot;Yes&quot;</span><span class="p">]</span>
    <span class="n">gt_answers</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Yes&quot;</span><span class="p">,</span> <span class="s2">&quot;Yes&quot;</span><span class="p">,</span> <span class="s2">&quot;No&quot;</span><span class="p">]</span>

    <span class="n">llm_judge</span> <span class="o">=</span> <span class="n">DefaultLLMJudge</span><span class="p">(</span>
        <span class="n">model_client</span><span class="o">=</span><span class="n">OpenAIClient</span><span class="p">(),</span>
        <span class="n">model_kwargs</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;gpt-4o&quot;</span><span class="p">,</span>
            <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
            <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">)</span>
    <span class="n">llm_evaluator</span> <span class="o">=</span> <span class="n">LLMasJudge</span><span class="p">(</span><span class="n">llm_judge</span><span class="o">=</span><span class="n">llm_judge</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">llm_judge</span><span class="p">)</span>
    <span class="n">eval_rslt</span> <span class="o">=</span> <span class="n">llm_evaluator</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span>
        <span class="n">questions</span><span class="o">=</span><span class="n">questions</span><span class="p">,</span> <span class="n">gt_answers</span><span class="o">=</span><span class="n">gt_answers</span><span class="p">,</span> <span class="n">pred_answers</span><span class="o">=</span><span class="n">pred_answers</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">eval_rslt</span><span class="p">)</span>
</pre></div>
</div>
<p>To ensure more rigor, you can compute a 95% confidence interval for the judgment score. When the evaluation dataset is small, the confidence interval may have a large range, indicating that the judgment score is not very reliable.</p>
<p>The output will be:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="err">LLMJudgeEvalResul</span><span class="kc">t</span><span class="err">(avg_score=</span><span class="mf">0.6666666666666666</span><span class="p">,</span><span class="w"> </span><span class="err">judgeme</span><span class="kc">nt</span><span class="err">_score_lis</span><span class="kc">t</span><span class="err">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="err">co</span><span class="kc">nf</span><span class="err">ide</span><span class="kc">n</span><span class="err">ce_i</span><span class="kc">nter</span><span class="err">val=(</span><span class="mf">0.013333333333333197</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="err">))</span>
</pre></div>
</div>
<p>This type of LLM judeg is seen in text-grad <a class="footnote-reference brackets" href="#id55" id="id26" role="doc-noteref"><span class="fn-bracket">[</span>17<span class="fn-bracket">]</span></a>.
You can view the prompt we used simply using <cite>print(llm_judge)</cite>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">DefaultLLMJudge</span><span class="p">(</span>
    <span class="n">judgement_query</span><span class="o">=</span> <span class="n">Does</span> <span class="n">the</span> <span class="n">predicted</span> <span class="n">answer</span> <span class="n">contain</span> <span class="n">the</span> <span class="n">ground</span> <span class="n">truth</span> <span class="n">answer</span><span class="err">?</span> <span class="n">Say</span> <span class="kc">True</span> <span class="k">if</span> <span class="n">yes</span><span class="p">,</span> <span class="kc">False</span> <span class="k">if</span> <span class="n">no</span><span class="o">.</span><span class="p">,</span>
    <span class="p">(</span><span class="n">model_client</span><span class="p">):</span> <span class="n">OpenAIClient</span><span class="p">()</span>
    <span class="p">(</span><span class="n">llm_evaluator</span><span class="p">):</span> <span class="n">Generator</span><span class="p">(</span>
        <span class="n">model_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="s1">&#39;gpt-4o&#39;</span><span class="p">,</span> <span class="s1">&#39;temperature&#39;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;max_tokens&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">},</span> <span class="n">trainable_prompt_kwargs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;task_desc_str&#39;</span><span class="p">,</span> <span class="s1">&#39;examples_str&#39;</span><span class="p">]</span>
        <span class="p">(</span><span class="n">prompt</span><span class="p">):</span> <span class="n">Prompt</span><span class="p">(</span>
        <span class="n">template</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">START_OF_SYSTEM_PROMPT</span><span class="o">&gt;</span>
        <span class="p">{</span><span class="c1"># task desc #}</span>
        <span class="p">{{</span><span class="n">task_desc_str</span><span class="p">}}</span>
        <span class="p">{</span><span class="c1"># examples #}</span>
        <span class="p">{</span><span class="o">%</span> <span class="k">if</span> <span class="n">examples_str</span> <span class="o">%</span><span class="p">}</span>
        <span class="p">{{</span><span class="n">examples_str</span><span class="p">}}</span>
        <span class="p">{</span><span class="o">%</span> <span class="n">endif</span> <span class="o">%</span><span class="p">}</span>
        <span class="o">&lt;</span><span class="n">END_OF_SYSTEM_PROMPT</span><span class="o">&gt;</span>
        <span class="o">---------------------</span>
        <span class="o">&lt;</span><span class="n">START_OF_USER</span><span class="o">&gt;</span>
        <span class="p">{</span><span class="c1"># question #}</span>
        <span class="p">{</span><span class="o">%</span> <span class="k">if</span> <span class="n">question_str</span> <span class="ow">is</span> <span class="n">defined</span> <span class="o">%</span><span class="p">}</span>
        <span class="n">Question</span><span class="p">:</span> <span class="p">{{</span><span class="n">question_str</span><span class="p">}}</span>
        <span class="p">{</span><span class="o">%</span> <span class="n">endif</span> <span class="o">%</span><span class="p">}</span>
        <span class="p">{</span><span class="c1"># ground truth answer #}</span>
        <span class="p">{</span><span class="o">%</span> <span class="k">if</span> <span class="n">gt_answer_str</span> <span class="ow">is</span> <span class="n">defined</span> <span class="o">%</span><span class="p">}</span>
        <span class="n">Ground</span> <span class="n">truth</span> <span class="n">answer</span><span class="p">:</span> <span class="p">{{</span><span class="n">gt_answer_str</span><span class="p">}}</span>
        <span class="p">{</span><span class="o">%</span> <span class="n">endif</span> <span class="o">%</span><span class="p">}</span>
        <span class="p">{</span><span class="c1"># predicted answer #}</span>
        <span class="n">Predicted</span> <span class="n">answer</span><span class="p">:</span> <span class="p">{{</span><span class="n">pred_answer_str</span><span class="p">}}</span>
        <span class="o">&lt;</span><span class="n">END_OF_USER</span><span class="o">&gt;</span>
        <span class="p">,</span> <span class="n">prompt_kwargs</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;task_desc_str&#39;</span><span class="p">:</span> <span class="s1">&#39;You are an evaluator. Given the question(optional), ground truth answer(optional), and predicted answer, Does the predicted answer contain the ground truth answer? Say True if yes, False if no.&#39;</span><span class="p">,</span> <span class="s1">&#39;examples_str&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">},</span> <span class="n">prompt_variables</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;task_desc_str&#39;</span><span class="p">,</span> <span class="s1">&#39;examples_str&#39;</span><span class="p">,</span> <span class="s1">&#39;pred_answer_str&#39;</span><span class="p">,</span> <span class="s1">&#39;question_str&#39;</span><span class="p">,</span> <span class="s1">&#39;gt_answer_str&#39;</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="p">(</span><span class="n">model_client</span><span class="p">):</span> <span class="n">OpenAIClient</span><span class="p">()</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>Case 2: Without Question</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_llm_as_judge_wo_questions</span><span class="p">():</span>
    <span class="kn">from</span> <span class="nn">adalflow.eval.llm_as_judge</span> <span class="kn">import</span> <span class="n">LLMasJudge</span><span class="p">,</span> <span class="n">DefaultLLMJudge</span>
    <span class="kn">from</span> <span class="nn">adalflow.components.model_client</span> <span class="kn">import</span> <span class="n">OpenAIClient</span>


    <span class="n">llm_judge</span> <span class="o">=</span> <span class="n">DefaultLLMJudge</span><span class="p">(</span>
        <span class="n">model_client</span><span class="o">=</span><span class="n">OpenAIClient</span><span class="p">(),</span>
        <span class="n">model_kwargs</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;gpt-4o&quot;</span><span class="p">,</span>
            <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
            <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="n">jugement_query</span><span class="o">=</span><span class="s2">&quot;Does the predicted answer means the same as the ground truth answer? Say True if yes, False if no.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">llm_evaluator</span> <span class="o">=</span> <span class="n">LLMasJudge</span><span class="p">(</span><span class="n">llm_judge</span><span class="o">=</span><span class="n">llm_judge</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">llm_judge</span><span class="p">)</span>
    <span class="n">eval_rslt</span> <span class="o">=</span> <span class="n">llm_evaluator</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">gt_answers</span><span class="o">=</span><span class="p">[</span><span class="n">gt</span><span class="p">],</span> <span class="n">pred_answers</span><span class="o">=</span><span class="p">[</span><span class="n">pred</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">eval_rslt</span><span class="p">)</span>
</pre></div>
</div>
<p>The output will be:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="err">LLMJudgeEvalResul</span><span class="kc">t</span><span class="err">(avg_score=</span><span class="mf">1.0</span><span class="p">,</span><span class="w"> </span><span class="err">judgeme</span><span class="kc">nt</span><span class="err">_score_lis</span><span class="kc">t</span><span class="err">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="w"> </span><span class="err">co</span><span class="kc">nf</span><span class="err">ide</span><span class="kc">n</span><span class="err">ce_i</span><span class="kc">nter</span><span class="err">val=(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="err">))</span>
</pre></div>
</div>
</section>
<section id="g-eval">
<h3>G_Eval<a class="headerlink" href="#g-eval" title="Link to this heading">#</a></h3>
<figure class="align-center" id="id65">
<a class="reference internal image-reference" href="../_images/G_eval_structure.png"><img alt="G-eval structure" src="../_images/G_eval_structure.png" style="width: 700px;" />
</a>
<figcaption>
<p><span class="caption-text">G-eval framework structure</span><a class="headerlink" href="#id65" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>If you have no reference text, you can also use G-eval <a class="footnote-reference brackets" href="#id49" id="id27" role="doc-noteref"><span class="fn-bracket">[</span>11<span class="fn-bracket">]</span></a> to evaluate the generated text on the fly.
G-eval provided a way to evaluate:</p>
<ul class="simple">
<li><p><cite>relevance</cite>: evaluates how relevant the summarized text to the source text.</p></li>
<li><p><cite>fluency</cite>: the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure.</p></li>
<li><p><cite>consistency</cite>: evaluates the collective quality of all sentences.</p></li>
<li><p><cite>coherence</cite>: evaluates the the factual alignment between the summary and the summarized source.</p></li>
</ul>
<p>In our library, we provides the prompt for task <cite>Summarization</cite> and <cite>Chatbot</cite> as default.
We also map the score to the range [0, 1] for the ease of optimization.</p>
<p>Here is the code snippet to compute the G-eval score:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_g_eval_summarization</span><span class="p">():</span>
    <span class="kn">from</span> <span class="nn">adalflow.eval.g_eval</span> <span class="kn">import</span> <span class="n">GEvalLLMJudge</span><span class="p">,</span> <span class="n">GEvalJudgeEvaluator</span><span class="p">,</span> <span class="n">NLGTask</span>

    <span class="n">model_kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;gpt-4o&quot;</span><span class="p">,</span>
        <span class="s2">&quot;n&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
        <span class="s2">&quot;top_p&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
        <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="n">g_eval</span> <span class="o">=</span> <span class="n">GEvalLLMJudge</span><span class="p">(</span>
        <span class="n">default_task</span><span class="o">=</span><span class="n">NLGTask</span><span class="o">.</span><span class="n">SUMMARIZATION</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">g_eval</span><span class="p">)</span>
    <span class="n">input_template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;Source Document: </span><span class="si">{source}</span>
<span class="s2">    Summary: </span><span class="si">{summary}</span>
<span class="s2">    &quot;&quot;&quot;</span>

    <span class="n">input_str</span> <span class="o">=</span> <span class="n">input_template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">source</span><span class="o">=</span><span class="s2">&quot;Paul Merson has restarted his row with Andros Townsend after the Tottenham midfielder was brought on with only seven minutes remaining in his team &#39;s 0-0 draw with Burnley on Sunday . &#39;Just been watching the game , did you miss the coach ? # RubberDub # 7minutes , &#39; Merson put on Twitter . Merson initially angered Townsend for writing in his Sky Sports column that &#39;if Andros Townsend can get in ( the England team ) then it opens it up to anybody . &#39; Paul Merson had another dig at Andros Townsend after his appearance for Tottenham against Burnley Townsend was brought on in the 83rd minute for Tottenham as they drew 0-0 against Burnley Andros Townsend scores England &#39;s equaliser in their 1-1 friendly draw with Italy in Turin on Tuesday night The former Arsenal man was proven wrong when Townsend hit a stunning equaliser for England against Italy and he duly admitted his mistake . &#39;It &#39;s not as though I was watching hoping he would n&#39;t score for England , I &#39;m genuinely pleased for him and fair play to him </span><span class="se">\u00e2\u20ac\u201c</span><span class="s2"> it was a great goal , &#39; Merson said . &#39;It &#39;s just a matter of opinion , and my opinion was that he got pulled off after half an hour at Manchester United in front of Roy Hodgson , so he should n&#39;t have been in the squad . &#39;When I &#39;m wrong , I hold my hands up . I do n&#39;t have a problem with doing that - I &#39;ll always be the first to admit when I &#39;m wrong . &#39; Townsend hit back at Merson on Twitter after scoring for England against Italy Sky Sports pundit Merson ( centre ) criticised Townsend &#39;s call-up to the England squad last week Townsend hit back at Merson after netting for England in Turin on Wednesday , saying &#39;Not bad for a player that should be &#39;nowhere near the squad &#39; ay @ PaulMerse ? &#39; Any bad feeling between the pair seemed to have passed but Merson was unable to resist having another dig at Townsend after Tottenham drew at Turf Moor .&quot;</span><span class="p">,</span>
        <span class="n">summary</span><span class="o">=</span><span class="s2">&quot;Paul merson was brought on with only seven minutes remaining in his team &#39;s 0-0 draw with burnley . Andros townsend scored the tottenham midfielder in the 89th minute . Paul merson had another dig at andros townsend after his appearance . The midfielder had been brought on to the england squad last week . Click here for all the latest arsenal news news .&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">g_evaluator</span> <span class="o">=</span> <span class="n">GEvalJudgeEvaluator</span><span class="p">(</span><span class="n">llm_judge</span><span class="o">=</span><span class="n">g_eval</span><span class="p">)</span>

    <span class="n">response</span> <span class="o">=</span> <span class="n">g_evaluator</span><span class="p">(</span><span class="n">input_strs</span><span class="o">=</span><span class="p">[</span><span class="n">input_str</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;response: </span><span class="si">{</span><span class="n">response</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The output will be:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="err">respo</span><span class="kc">nse</span><span class="p">:</span><span class="w"> </span><span class="err">(</span><span class="p">{</span><span class="err">&#39;Releva</span><span class="kc">n</span><span class="err">ce&#39;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.4</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;Flue</span><span class="kc">n</span><span class="err">cy&#39;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.3333333333333333</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;Co</span><span class="kc">ns</span><span class="err">is</span><span class="kc">ten</span><span class="err">cy&#39;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.2</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;Cohere</span><span class="kc">n</span><span class="err">ce&#39;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.4</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;overall&#39;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.33333333333333337</span><span class="p">},</span><span class="w"> </span><span class="p">[{</span><span class="err">&#39;Releva</span><span class="kc">n</span><span class="err">ce&#39;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.4</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;Flue</span><span class="kc">n</span><span class="err">cy&#39;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.3333333333333333</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;Co</span><span class="kc">ns</span><span class="err">is</span><span class="kc">ten</span><span class="err">cy&#39;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.2</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;Cohere</span><span class="kc">n</span><span class="err">ce&#39;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.4</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;overall&#39;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.33333333333333337</span><span class="p">}]</span><span class="err">)</span>
</pre></div>
</div>
<p><cite>print(g_eval)</cite> will be:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">GEvalLLMJudge</span><span class="p">(</span>
    <span class="n">default_task</span><span class="o">=</span> <span class="n">NLGTask</span><span class="o">.</span><span class="n">SUMMARIZATION</span><span class="p">,</span> <span class="n">prompt_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;Relevance&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;task_desc_str&#39;</span><span class="p">:</span> <span class="s1">&#39;You will be given a summary of a text.  Please evaluate the summary based on the following criteria:&#39;</span><span class="p">,</span> <span class="s1">&#39;evaluation_criteria_str&#39;</span><span class="p">:</span> <span class="s1">&#39;Relevance (1-5) - selection of important content from the source.</span><span class="se">\n</span><span class="s1">        The summary should include only important information from the source document.</span><span class="se">\n</span><span class="s1">        Annotators were instructed to penalize summaries which contained redundancies and excess information.&#39;</span><span class="p">,</span> <span class="s1">&#39;evaluation_steps_str&#39;</span><span class="p">:</span> <span class="s1">&#39;1. Read the summary and the source document carefully.</span><span class="se">\n</span><span class="s1">        2. Compare the summary to the source document and identify the main points of the article.</span><span class="se">\n</span><span class="s1">        3. Assess how well the summary covers the main points of the article, and how much irrelevant or redundant information it contains.</span><span class="se">\n</span><span class="s1">        4. Assign a relevance score from 1 to 5.&#39;</span><span class="p">,</span> <span class="s1">&#39;metric_name&#39;</span><span class="p">:</span> <span class="s1">&#39;Relevance&#39;</span><span class="p">},</span> <span class="s1">&#39;Fluency&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;task_desc_str&#39;</span><span class="p">:</span> <span class="s1">&#39;You will be given a summary of a text.  Please evaluate the summary based on the following criteria:&#39;</span><span class="p">,</span> <span class="s1">&#39;evaluation_criteria_str&#39;</span><span class="p">:</span> <span class="s1">&#39;Fluency (1-3): the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure.</span><span class="se">\n</span><span class="s1">        - 1: Poor. The summary has many errors that make it hard to understand or sound unnatural.</span><span class="se">\n</span><span class="s1">        - 2: Fair. The summary has some errors that affect the clarity or smoothness of the text, but the main points are still comprehensible.</span><span class="se">\n</span><span class="s1">        - 3: Good. The summary has few or no errors and is easy to read and follow.</span><span class="se">\n</span><span class="s1">        &#39;</span><span class="p">,</span> <span class="s1">&#39;evaluation_steps_str&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;metric_name&#39;</span><span class="p">:</span> <span class="s1">&#39;Fluency&#39;</span><span class="p">},</span> <span class="s1">&#39;Consistency&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;task_desc_str&#39;</span><span class="p">:</span> <span class="s1">&#39;You will be given a summary of a text.  Please evaluate the summary based on the following criteria:&#39;</span><span class="p">,</span> <span class="s1">&#39;evaluation_criteria_str&#39;</span><span class="p">:</span> <span class="s1">&#39;Consistency (1-5) - the factual alignment between the summary and the summarized source.</span><span class="se">\n</span><span class="s1">        A factually consistent summary contains only statements that are entailed by the source document.</span><span class="se">\n</span><span class="s1">        Annotators were also asked to penalize summaries that contained hallucinated facts. &#39;</span><span class="p">,</span> <span class="s1">&#39;evaluation_steps_str&#39;</span><span class="p">:</span> <span class="s1">&#39;1. Read the summary and the source document carefully.</span><span class="se">\n</span><span class="s1">        2. Identify the main facts and details it presents.</span><span class="se">\n</span><span class="s1">        3. Read the summary and compare it to the source document to identify any inconsistencies or factual errors that are not supported by the source.</span><span class="se">\n</span><span class="s1">        4. Assign a score for consistency based on the Evaluation Criteria.&#39;</span><span class="p">,</span> <span class="s1">&#39;metric_name&#39;</span><span class="p">:</span> <span class="s1">&#39;Consistency&#39;</span><span class="p">},</span> <span class="s1">&#39;Coherence&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;task_desc_str&#39;</span><span class="p">:</span> <span class="s1">&#39;You will be given a summary of a text.  Please evaluate the summary based on the following criteria:&#39;</span><span class="p">,</span> <span class="s1">&#39;evaluation_criteria_str&#39;</span><span class="p">:</span> <span class="s1">&#39;Coherence (1-5) - the collective quality of all sentences.</span><span class="se">\n</span><span class="s1">        We align this dimension with the DUC quality question of structure and coherence whereby &quot;the summary should be well-structured and well-organized.</span><span class="se">\n</span><span class="s1">        The summary should not just be a heap of related information, but should build from sentence to a coherent body of information about a topic.&#39;</span><span class="p">,</span> <span class="s1">&#39;evaluation_steps_str&#39;</span><span class="p">:</span> <span class="s1">&#39;1. Read the input text carefully and identify the main topic and key points.</span><span class="se">\n</span><span class="s1">        2. Read the summary and assess how well it captures the main topic and key points. And if it presents them in a clear and logical order.</span><span class="se">\n</span><span class="s1">        3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.&#39;</span><span class="p">,</span> <span class="s1">&#39;metric_name&#39;</span><span class="p">:</span> <span class="s1">&#39;Coherence&#39;</span><span class="p">}}</span>
    <span class="p">(</span><span class="n">model_client</span><span class="p">):</span> <span class="n">OpenAIClient</span><span class="p">()</span>
    <span class="p">(</span><span class="n">llm_evaluator</span><span class="p">):</span> <span class="n">Generator</span><span class="p">(</span>
        <span class="n">model_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="s1">&#39;gpt-4o&#39;</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span> <span class="s1">&#39;top_p&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;max_tokens&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> <span class="s1">&#39;temperature&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span> <span class="n">trainable_prompt_kwargs</span><span class="o">=</span><span class="p">[]</span>
        <span class="p">(</span><span class="n">prompt</span><span class="p">):</span> <span class="n">Prompt</span><span class="p">(</span>
        <span class="n">template</span><span class="p">:</span>
        <span class="o">&lt;</span><span class="n">START_OF_SYSTEM_PROMPT</span><span class="o">&gt;</span>
        <span class="p">{</span><span class="c1"># task desc #}</span>
        <span class="p">{{</span><span class="n">task_desc_str</span><span class="p">}}</span>
        <span class="o">---------------------</span>
        <span class="p">{</span><span class="c1"># evaluation criteria #}</span>
        <span class="n">Evaluation</span> <span class="n">Criteria</span><span class="p">:</span>
        <span class="p">{{</span><span class="n">evaluation_criteria_str</span><span class="p">}}</span>
        <span class="o">---------------------</span>
        <span class="p">{</span><span class="c1"># evaluation steps #}</span>
        <span class="p">{</span><span class="o">%</span> <span class="k">if</span> <span class="n">evaluation_steps_str</span> <span class="o">%</span><span class="p">}</span>
        <span class="n">Evaluation</span> <span class="n">Steps</span><span class="p">:</span>
        <span class="p">{{</span><span class="n">evaluation_steps_str</span><span class="p">}}</span>
        <span class="o">---------------------</span>
        <span class="p">{</span><span class="o">%</span> <span class="n">endif</span> <span class="o">%</span><span class="p">}</span>
        <span class="p">{{</span><span class="n">input_str</span><span class="p">}}</span>
        <span class="p">{</span> <span class="c1"># evaluation form #}</span>
        <span class="n">Evaluation</span> <span class="n">Form</span> <span class="p">(</span><span class="n">scores</span> <span class="n">ONLY</span><span class="p">):</span>
        <span class="o">-</span> <span class="p">{{</span><span class="n">metric_name</span><span class="p">}}:</span>

        <span class="n">Output</span> <span class="n">the</span> <span class="n">score</span> <span class="n">only</span><span class="o">.</span>
        <span class="o">&lt;</span><span class="n">END_OF_SYSTEM_PROMPT</span><span class="o">&gt;</span>
        <span class="p">,</span> <span class="n">prompt_variables</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;input_str&#39;</span><span class="p">,</span> <span class="s1">&#39;task_desc_str&#39;</span><span class="p">,</span> <span class="s1">&#39;evaluation_criteria_str&#39;</span><span class="p">,</span> <span class="s1">&#39;evaluation_steps_str&#39;</span><span class="p">,</span> <span class="s1">&#39;metric_name&#39;</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="p">(</span><span class="n">model_client</span><span class="p">):</span> <span class="n">OpenAIClient</span><span class="p">()</span>
        <span class="p">(</span><span class="n">output_processors</span><span class="p">):</span> <span class="n">FloatParser</span><span class="p">()</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="train-align-llm-judge">
<h3>Train/Align LLM Judge<a class="headerlink" href="#train-align-llm-judge" title="Link to this heading">#</a></h3>
<p>We should better align the LLM judge with a human preference dataset that contains (generated text, ground truth text, score) triplets.
This process is the same as optimizinh the task pipeline, where you can create an <code class="docutils literal notranslate"><span class="pre">AdalComponent</span></code> and call our <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> to do the in-context learning.
From the printout, you can observe the two trainable_prompt_kwargs in the <code class="docutils literal notranslate"><span class="pre">DefaultLLMJudge</span></code>.</p>
<p>In this case, we may want to compute a correlation score between the human judge and the LLM judge.
You have various options, such as:</p>
<ol class="arabic simple">
<li><p>Pearson Correlation Coefficient</p></li>
<li><p>Kendallrank correlation coefficient from ARES <a class="footnote-reference brackets" href="#id52" id="id28" role="doc-noteref"><span class="fn-bracket">[</span>14<span class="fn-bracket">]</span></a>, particularly useful for ranking systems (Retrieval).</p></li>
</ol>
</section>
</section>
<section id="rag-evaluation">
<h2>RAG Evaluation<a class="headerlink" href="#rag-evaluation" title="Link to this heading">#</a></h2>
<p>RAG (Retrieval-Augmented Generation) pipelines are a combination of a retriever and a generator.
The retriever retrieves relevant context from a large corpus, and the generator generates the final answer based on the retrieved context.
When a retriever failed to retrieve relevant context, the generator may fail.
Therefore, besides of evaluating RAG pipelines as a whole using NLG metrics, it is also important to evaluate the retriever and to optimize the evalulation metrics from both stages to best improve the final performance.</p>
<section id="with-gt-for-retriever">
<h3>With GT for Retriever<a class="headerlink" href="#with-gt-for-retriever" title="Link to this heading">#</a></h3>
<p>For the retriever, the metrics used are nothing new but from the standard information retrieval/ranking literature.
Often, we have</p>
<ol class="arabic simple">
<li><p>Recall&#64;k: the proportion of relevant documents that are retrieved out of the total number of relevant documents.</p></li>
<li><p>Mean Reciprocal Rank(MRR&#64;k), HitRate&#64;k, etc.</p></li>
<li><p>NDCG&#64;k</p></li>
<li><p>Precision&#64;k, MAP&#64;k etc.</p></li>
</ol>
<p>For defails of these metrics, please refer to <a class="footnote-reference brackets" href="#id56" id="id29" role="doc-noteref"><span class="fn-bracket">[</span>18<span class="fn-bracket">]</span></a>.
All of these metrics, you can also find at <a class="reference external" href="https://lightning.ai/docs/torchmetrics/stable/">TorchMetrics</a>.</p>
<p>For example, you can use the following code snippet to compute the recall&#64;k the retriever component of the RAG pipeline for a single query if
the ground truth context is provided.
In this example, the retrieved contexts is a joined string of the retrieved context chunks, and the gt_contexts is a list of ground truth context chunks for each query.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">adalflow.eval</span> <span class="kn">import</span> <span class="n">RetrieverRecall</span><span class="p">,</span> <span class="n">RetrieverRelevance</span>

<span class="n">retrieved_contexts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;Apple is founded before Google.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Feburary has 28 days in common years. Feburary has 29 days in leap years. Feburary is the second month of the year.&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">gt_contexts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span>
        <span class="s2">&quot;Apple is founded in 1976.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Google is founded in 1998.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Apple is founded before Google.&quot;</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="p">[</span><span class="s2">&quot;Feburary has 28 days in common years&quot;</span><span class="p">,</span> <span class="s2">&quot;Feburary has 29 days in leap years&quot;</span><span class="p">],</span>
<span class="p">]</span>
<span class="n">retriever_recall</span> <span class="o">=</span> <span class="n">RetrieverRecall</span><span class="p">()</span>
<span class="n">avg_recall</span><span class="p">,</span> <span class="n">recall_list</span> <span class="o">=</span> <span class="n">retriever_recall</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">retrieved_contexts</span><span class="p">,</span> <span class="n">gt_contexts</span><span class="p">)</span> <span class="c1"># Compute the recall of the retriever</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Recall: </span><span class="si">{</span><span class="n">avg_recall</span><span class="si">}</span><span class="s2">, Recall List: </span><span class="si">{</span><span class="n">recall_list</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The output will be:</p>
<p>For the first query, only one out of three relevant documents is retrieved, resulting in a recall of 0.33.
For the second query, all relevant documents are retrieved, resulting in a recall of 1.0.</p>
</section>
<section id="without-gt-contexts">
<h3>Without gt_contexts<a class="headerlink" href="#without-gt-contexts" title="Link to this heading">#</a></h3>
<section id="id31">
<h4>RAGAS<a class="headerlink" href="#id31" title="Link to this heading">#</a></h4>
<p>Ideally, for each query, we will retrieve the top k (&#64;k) chunks and to get the above score, we expect each query, retrieved chunk pair comes with a ground truth labeling.
But this is highly unrealistic especially if corpora is large.
If we have 100 test queries, and a corpus of size 1000 chunks, the pairs we need to annoate is 10^5.
There are different strategies to handle this problem but we could not dive into all of them here.</p>
<p>There is one new way is to indirectly use the ground truth answers from the generator to evaluate the retriever.
<a class="reference external" href="https://docs.ragas.io/en/stable/getstarted/index.html">RAGAS</a> framework provides one way to do this.</p>
<blockquote>
<div><p>Recall = [GT statements that can be attributed to the retrieved context] / [GT statements]</p>
</div></blockquote>
<p>There is also <strong>Context Relevance</strong> and <strong>Context Precision</strong> metrics in RAGAS.</p>
</section>
<section id="llm-or-model-based-judge-for-retriever-recall">
<h4>LLM or model based judge for Retriever Recall<a class="headerlink" href="#llm-or-model-based-judge-for-retriever-recall" title="Link to this heading">#</a></h4>
<p><strong>LLM judge with in-context prompting</strong></p>
<p>LLM judge to directly straightforward way to evaluate the top k score on the fly.</p>
<p>We can create a subset of query, retrieved chunk pairs and manually label them, and we train an LLM judge to predict the score.
If the judge can achieve a high accuracy then we are able to annotate any metric in the retriever given the query and the retrieved chunk pairs.</p>
<p><strong>ARES with finetuned classifier with synthetic data</strong></p>
<p>ARES <a class="footnote-reference brackets" href="#id52" id="id33" role="doc-noteref"><span class="fn-bracket">[</span>14<span class="fn-bracket">]</span></a> proposed to create a synthetic dataset from an in-domain corpora.
The generated data represent both positive and negative examples of <cite>query–passage–answer triples`</cite> (e.g.,relevant/irrelevant passages and correct/incorrectanswers).</p>
<p>The synthetic dataset is used to train a classifier consists of  embedding and a classification head.
It claims to be able to adapt to other domains where the classifier is not trained on.
The cost of this approach is quite low as you can compute the embedding for only once for each query and each chunk in the corpus.</p>
<p><strong>RAGEval for vertical domain evaluation</strong></p>
<p>RAGEVal <a class="footnote-reference brackets" href="#id59" id="id34" role="doc-noteref"><span class="fn-bracket">[</span>21<span class="fn-bracket">]</span></a> proposed a framework to synthesize vertical domain evaluation dataset such as finance, healthcare, legal etc where due to the privacy, it is challenging to create a large real-world dataset.</p>
<p><strong>More</strong></p>
<p>See the evaluation on datasets at <span class="xref std std-doc">Evaluating a RAG Pipeline</span>.</p>
<p>Additionally, there are more research for RAG evaluation, such as SemScore <a class="footnote-reference brackets" href="#id51" id="id35" role="doc-noteref"><span class="fn-bracket">[</span>13<span class="fn-bracket">]</span></a>, ARES <a class="footnote-reference brackets" href="#id52" id="id36" role="doc-noteref"><span class="fn-bracket">[</span>14<span class="fn-bracket">]</span></a>, RGB <a class="footnote-reference brackets" href="#id53" id="id37" role="doc-noteref"><span class="fn-bracket">[</span>15<span class="fn-bracket">]</span></a>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>GovTech Singapore provides a well-explained evaluation guideline <a class="footnote-reference brackets" href="#id60" id="id38" role="doc-noteref"><span class="fn-bracket">[</span>22<span class="fn-bracket">]</span></a> that aligns with our guideline but with more thereotical explanation on some metrics.</p>
</div>
</section>
</section>
</section>
<section id="for-contributors">
<h2>For Contributors<a class="headerlink" href="#for-contributors" title="Link to this heading">#</a></h2>
<p>There are way too many metrics and evaluation methods that AdalFlow can cover in the library.
We encourage contributors who work on evaluation research and production to build evaluator that is compatible with AdalFlow.
This means that:</p>
<ol class="arabic simple">
<li><p>The evaluator can potentially output a single float score in range [0, 1] so that AdalFlow Trainer can use it to optimize the pipeline.</p></li>
<li><p>For using LLM as judge, the judge should be built similar to <cite>DefaultLLMJudge</cite> so that there are trainable_prompt_kwargs that users can further align the judge with human preference dataset.</p></li>
</ol>
<p>For instance, for the research papers we have listed here, it would be great to have a version that is easily compatible with AdalFlow.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id39" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id2">2</a>,<a role="doc-backlink" href="#id3">3</a>,<a role="doc-backlink" href="#id11">4</a>,<a role="doc-backlink" href="#id15">5</a>)</span>
<p>Chang, Yupeng, et al. “A survey on evaluation of large language models.” ACM Transactions on Intelligent Systems and Technology 15.3 (2024): 1-45.</p>
</aside>
<aside class="footnote brackets" id="id40" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id4">1</a>,<a role="doc-backlink" href="#id12">2</a>)</span>
<p>Guo, Zishan, et al. “Evaluating large language models: A comprehensive survey.” arXiv preprint arXiv:2310.19736 (2023).</p>
</aside>
<aside class="footnote brackets" id="id41" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">3</a><span class="fn-bracket">]</span></span>
<p>Achiam, Josh, et al. “GPT-4 technical report.” arXiv preprint arXiv:2303.08774 (2023).</p>
</aside>
<aside class="footnote brackets" id="id42" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id8">1</a>,<a role="doc-backlink" href="#id9">2</a>)</span>
<p>Hendrycks, Dan, et al. “Measuring massive multitask language understanding.” International Conference on Learning Representations. 2020.</p>
</aside>
<aside class="footnote brackets" id="id43" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">5</a><span class="fn-bracket">]</span></span>
<p>Chen, Mark, et al. “Evaluating large language models trained on code.” arXiv preprint arXiv:2107.03374 (2021).</p>
</aside>
<aside class="footnote brackets" id="id44" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id13">1</a>,<a role="doc-backlink" href="#id16">2</a>)</span>
<p>Liu, Yang, et al. “Datasets for Large Language Models: A Comprehensive Survey.” arXiv preprint arXiv:2402.18041 (2024).</p>
</aside>
<aside class="footnote brackets" id="id45" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></span>
<p>Finardi, Paulo, et al. “The Chronicles of RAG: The Retriever, the Chunk and the Generator.” arXiv preprint arXiv:2401.07883 (2024).</p>
</aside>
<aside class="footnote brackets" id="id46" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id18">8</a><span class="fn-bracket">]</span></span>
<ol class="upperalpha simple" start="11">
<li><p>Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for automatic evaluation of machine transla-tion,” in Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational Linguistics, 2002, pp. 311–318.</p></li>
</ol>
</aside>
<aside class="footnote brackets" id="id47" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id19">9</a><span class="fn-bracket">]</span></span>
<p>C.-Y. Lin, “Rouge: a package for automatic evaluation of summaries,” 2004.</p>
</aside>
<aside class="footnote brackets" id="id48" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id22">10</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://lightning.ai/docs/torchmetrics/stable/text/rouge_score.html">https://lightning.ai/docs/torchmetrics/stable/text/rouge_score.html</a></p>
</aside>
<aside class="footnote brackets" id="id49" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id27">11</a><span class="fn-bracket">]</span></span>
<ol class="upperalpha simple" start="25">
<li><p>Liu, D. Iter, Y. Xu, S. Wang, R. Xu, and C. Zhu, “G-eval: Nlg evaluation using gpt-4 with better humanalignment,” 2023.</p></li>
</ol>
</aside>
<aside class="footnote brackets" id="id50" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id21">12</a><span class="fn-bracket">]</span></span>
<p>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: Anautomatic metric for mt evaluation with improved cor-relation with human judgments. In Proceedings ofthe acl workshop on intrinsic and extrinsic evaluationmeasures for machine translation and/or summariza-tion, pages 65–72.</p>
</aside>
<aside class="footnote brackets" id="id51" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id35">13</a><span class="fn-bracket">]</span></span>
<p>SemScore: <a class="reference external" href="https://arxiv.org/abs/2401.17072">https://arxiv.org/abs/2401.17072</a></p>
</aside>
<aside class="footnote brackets" id="id52" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>14<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id28">1</a>,<a role="doc-backlink" href="#id33">2</a>,<a role="doc-backlink" href="#id36">3</a>)</span>
<p>ARES: <a class="reference external" href="https://arxiv.org/abs/2311.09476">https://arxiv.org/abs/2311.09476</a>, <a class="github reference external" href="https://github.com/stanford-futuredata/ARES">stanford-futuredata/ARES</a></p>
</aside>
<aside class="footnote brackets" id="id53" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id37">15</a><span class="fn-bracket">]</span></span>
<p>RGB: <a class="reference external" href="https://ojs.aaai.org/index.php/AAAI/article/view/29728">https://ojs.aaai.org/index.php/AAAI/article/view/29728</a></p>
</aside>
<aside class="footnote brackets" id="id54" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>16<span class="fn-bracket">]</span></span>
<p>G-eval: <a class="github reference external" href="https://github.com/nlpyang/geval">nlpyang/geval</a></p>
</aside>
<aside class="footnote brackets" id="id55" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id26">17</a><span class="fn-bracket">]</span></span>
<p>Text-grad: <a class="reference external" href="https://arxiv.org/abs/2309.03409">https://arxiv.org/abs/2309.03409</a></p>
</aside>
<aside class="footnote brackets" id="id56" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id29">18</a><span class="fn-bracket">]</span></span>
<p>Pretrained Transformers for Text Ranking: BERT and Beyond: <a class="reference external" href="https://arxiv.org/pdf/2010.06467">https://arxiv.org/pdf/2010.06467</a></p>
</aside>
<aside class="footnote brackets" id="id57" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">19</a><span class="fn-bracket">]</span></span>
<p>Liu, Yang, et al. “Datasets for large language models: A comprehensive survey.” arXiv preprint arXiv:2402.18041 (2024).</p>
</aside>
<aside class="footnote brackets" id="id58" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id20">20</a><span class="fn-bracket">]</span></span>
<p>ROUGE Deep dive: <a class="reference external" href="https://medium.com/nlplanet/two-minutes-nlp-learn-the-rouge-metric-by-examples-f179cc285499">https://medium.com/nlplanet/two-minutes-nlp-learn-the-rouge-metric-by-examples-f179cc285499</a></p>
</aside>
<aside class="footnote brackets" id="id59" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>21<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id5">1</a>,<a role="doc-backlink" href="#id14">2</a>,<a role="doc-backlink" href="#id34">3</a>)</span>
<p>Zhu, Kunlun, et al. “RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework.” arXiv preprint arXiv:2408.01262 (2024).</p>
</aside>
<aside class="footnote brackets" id="id60" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id38">22</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://playbooks.capdev.govtext.gov.sg/evaluation/">https://playbooks.capdev.govtext.gov.sg/evaluation/</a></p>
</aside>
</aside>
<div class="highlight admonition">
<p class="admonition-title">AdalFlow Eval API Reference</p>
<ul class="simple">
<li><p><a class="reference internal" href="../apis/eval/eval.retriever_recall.html#module-eval.retriever_recall" title="eval.retriever_recall"><code class="xref py py-class docutils literal notranslate"><span class="pre">RetrieverRecall</span></code></a></p></li>
<li><p><a class="reference internal" href="../apis/eval/eval.llm_as_judge.html#module-eval.llm_as_judge" title="eval.llm_as_judge"><code class="xref py py-class docutils literal notranslate"><span class="pre">DefaultLLMJudge</span></code></a></p></li>
<li><p><a class="reference internal" href="../apis/eval/eval.answer_match_acc.html#module-eval.answer_match_acc" title="eval.answer_match_acc"><code class="xref py py-class docutils literal notranslate"><span class="pre">AnswerMatchAcc</span></code></a></p></li>
<li><p><a class="reference internal" href="../apis/eval/eval.g_eval.html#module-eval.g_eval" title="eval.g_eval"><code class="xref py py-class docutils literal notranslate"><span class="pre">GEvalLLMJudge</span></code></a></p></li>
<li><p><a class="reference internal" href="../apis/eval/eval.g_eval.html#module-eval.g_eval" title="eval.g_eval"><code class="xref py py-class docutils literal notranslate"><span class="pre">GEvalJudgeEvaluator</span></code></a></p></li>
</ul>
</div>
<div class="highlight admonition">
<p class="admonition-title">Other Evaluation Metrics libraries</p>
<ul class="simple">
<li><p><a class="reference external" href="https://lightning.ai/docs/torchmetrics">TorchMetrics</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/metrics">Hugging Face Metrics</a></p></li>
<li><p><a class="reference external" href="https://docs.ragas.io/en/stable/getstarted/index.html">RAGAS</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2303.08774">G-eval</a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/model_evaluation.html">Sklearn</a></p></li>
</ul>
</div>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="agent.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Agent</p>
      </div>
    </a>
    <a class="right-next"
       href="datasets.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Datasets</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tasks-and-capabilities">Tasks and Capabilities</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#datasets-and-benchmarks">Datasets and Benchmarks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-metrics">Evaluation Metrics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nlg-evaluation">NLG Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classicial-string-metrics">Classicial String Metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-based-metrics">Embedding-based Metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-as-judge">LLM as Judge</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#g-eval">G_Eval</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-align-llm-judge">Train/Align LLM Judge</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rag-evaluation">RAG Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#with-gt-for-retriever">With GT for Retriever</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#without-gt-contexts">Without gt_contexts</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id31">RAGAS</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-or-model-based-judge-for-retriever-recall">LLM or model based judge for Retriever Recall</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#for-contributors">For Contributors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2024, SylphAI, Inc.
      <br/>
    
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>