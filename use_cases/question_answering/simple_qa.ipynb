{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build A Simple Question-Answering Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this use case, we show how to build a simple question-answering pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed modules from LightRAG\n",
    "from adalflow.core.component import Component\n",
    "from adalflow.core.generator import Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we use the OpenAIClient as an example, but you can use any other clients (with the corresponding API Key as needed), such as AnthropicAPIClient\n",
    "from adalflow.components.model_client import OpenAIClient\n",
    "OPENAI_API_KEY=\"YOUR_API_KEY\" # Replace with your OpenAI API Key, or you can put it in a .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleQA(\n",
      "  (generator): Generator(\n",
      "    model_kwargs={'model': 'gpt-3.5-turbo'}, \n",
      "    (prompt): Prompt(\n",
      "      template: \n",
      "      {% if task_desc_str or output_format_str or tools_str or examples_str or chat_history_str or context_str or steps_str %}\n",
      "      <SYS>\n",
      "      {% endif %}\n",
      "      {# task desc #}\n",
      "      {% if task_desc_str %}\n",
      "      {{task_desc_str}}\n",
      "      {% endif %}\n",
      "      {# output format #}\n",
      "      {% if output_format_str %}\n",
      "      <OUTPUT_FORMAT>\n",
      "      {{output_format_str}}\n",
      "      </OUTPUT_FORMAT>\n",
      "      {% endif %}\n",
      "      {# tools #}\n",
      "      {% if tools_str %}\n",
      "      <TOOLS>\n",
      "      {{tools_str}}\n",
      "      </TOOLS>\n",
      "      {% endif %}\n",
      "      {# example #}\n",
      "      {% if examples_str %}\n",
      "      <EXAMPLES>\n",
      "      {{examples_str}}\n",
      "      </EXAMPLES>\n",
      "      {% endif %}\n",
      "      {# chat history #}\n",
      "      {% if chat_history_str %}\n",
      "      <CHAT_HISTORY>\n",
      "      {{chat_history_str}}\n",
      "      </CHAT_HISTORY>\n",
      "      {% endif %}\n",
      "      {#contex#}\n",
      "      {% if context_str %}\n",
      "      <CONTEXT>\n",
      "      {{context_str}}\n",
      "      </CONTEXT>\n",
      "      {% endif %}\n",
      "      {# steps #}\n",
      "      {% if steps_str %}\n",
      "      <STEPS>\n",
      "      {{steps_str}}\n",
      "      </STEPS>\n",
      "      {% endif %}\n",
      "      {% if task_desc_str or output_format_str or tools_str or examples_str or chat_history_str or context_str or steps_str %}\n",
      "      </SYS>\n",
      "      {% endif %}\n",
      "      {% if input_str %}\n",
      "      <Inputs>\n",
      "      {{input_str}}\n",
      "      </Inputs>\n",
      "      {% endif %}\n",
      "      {% if output_str %}\n",
      "      <Outputs>\n",
      "      {{output_str}}\n",
      "      </Outputs>\n",
      "      {% endif %}\n",
      "      You:\n",
      "      , prompt_variables: ['task_desc_str', 'steps_str', 'output_str', 'chat_history_str', 'tools_str', 'output_format_str', 'examples_str', 'context_str', 'input_str']\n",
      "    )\n",
      "    (model_client): OpenAIClient()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Build the SimpleQA pipeline\n",
    "class SimpleQA(Component):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generator = Generator(\n",
    "            model_client=OpenAIClient(),\n",
    "            model_kwargs={'model': 'gpt-3.5-turbo'}\n",
    "        )\n",
    "\n",
    "    def call(self, query: str):\n",
    "        return self.generator.call(prompt_kwargs={'input_str': query})\n",
    "\n",
    "simple_qa = SimpleQA()\n",
    "print(simple_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeneratorOutput(data='The capital of France is Paris.', error=None, usage=None, raw_response='The capital of France is Paris.')\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the capital of France?\"\n",
    "response = simple_qa.call(query)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightrag-project",
   "language": "python",
   "name": "light-rag-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
