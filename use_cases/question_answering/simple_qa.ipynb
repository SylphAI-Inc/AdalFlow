{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build A Simple Question-Answering Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this use case, we show how to build a simple question-answering pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed modules from LightRAG\n",
    "from adalflow.core.component import Component\n",
    "from adalflow.core.generator import Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we use the OpenAIClient as an example, but you can use any other clients (with the corresponding API Key as needed), such as AnthropicAPIClient\n",
    "from adalflow.utils import (\n",
    "    setup_env,\n",
    ")  # make sure you have a .env file with OPENAI_API_KEY or any other key mentioned with respect to your usage\n",
    "\n",
    "setup_env(\".env\")\n",
    "from adalflow.components.model_client import OpenAIClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleQA(\n",
      "  (generator): Generator(\n",
      "    model_kwargs={'model': 'gpt-3.5-turbo'}, trainable_prompt_kwargs=[]\n",
      "    (prompt): Prompt(\n",
      "      template: <START_OF_SYSTEM_PROMPT>\n",
      "      {# task desc #}\n",
      "      {% if task_desc_str %}\n",
      "      {{task_desc_str}}\n",
      "      {% else %}\n",
      "      You are a helpful assistant.\n",
      "      {% endif %}\n",
      "      {#input format#}\n",
      "      {% if input_format_str %}\n",
      "      <INPUT_FORMAT>\n",
      "      {{input_format_str}}\n",
      "      </INPUT_FORMAT>\n",
      "      {% endif %}\n",
      "      {# output format #}\n",
      "      {% if output_format_str %}\n",
      "      <OUTPUT_FORMAT>\n",
      "      {{output_format_str}}\n",
      "      </OUTPUT_FORMAT>\n",
      "      {% endif %}\n",
      "      {# tools #}\n",
      "      {% if tools_str %}\n",
      "      <TOOLS>\n",
      "      {{tools_str}}\n",
      "      </TOOLS>\n",
      "      {% endif %}\n",
      "      {# example #}\n",
      "      {% if examples_str %}\n",
      "      <EXAMPLES>\n",
      "      {{examples_str}}\n",
      "      </EXAMPLES>\n",
      "      {% endif %}\n",
      "      {# chat history #}\n",
      "      {% if chat_history_str %}\n",
      "      <CHAT_HISTORY>\n",
      "      {{chat_history_str}}\n",
      "      </CHAT_HISTORY>\n",
      "      {% endif %}\n",
      "      {#contex#}\n",
      "      {% if context_str %}\n",
      "      <CONTEXT>\n",
      "      {{context_str}}\n",
      "      </CONTEXT>\n",
      "      {% endif %}\n",
      "      <END_OF_SYSTEM_PROMPT>\n",
      "      <START_OF_USER_PROMPT>\n",
      "      {% if input_str %}\n",
      "      {{input_str}}\n",
      "      {% endif %}\n",
      "      <END_OF_USER_PROMPT>\n",
      "      {# steps #}\n",
      "      {% if steps_str %}\n",
      "      <START_OF_ASSISTANT_STEPS>\n",
      "      {{steps_str}}\n",
      "      <END_OF_ASSISTANT_STEPS>\n",
      "      {% endif %}\n",
      "      , prompt_variables: ['output_format_str', 'tools_str', 'chat_history_str', 'context_str', 'input_str', 'examples_str', 'task_desc_str', 'input_format_str', 'steps_str']\n",
      "    )\n",
      "    (model_client): OpenAIClient()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Build the SimpleQA pipeline\n",
    "class SimpleQA(Component):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generator = Generator(\n",
    "            model_client=OpenAIClient(), model_kwargs={\"model\": \"gpt-3.5-turbo\"}\n",
    "        )\n",
    "\n",
    "    def call(self, query: str):\n",
    "        return self.generator.call(prompt_kwargs={\"input_str\": query})\n",
    "\n",
    "\n",
    "simple_qa = SimpleQA()\n",
    "print(simple_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeneratorOutput(id=None, data='The capital of France is Paris.', error=None, usage=CompletionUsage(completion_tokens=7, prompt_tokens=48, total_tokens=55), raw_response='The capital of France is Paris.', metadata=None)\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the capital of France?\"\n",
    "response = simple_qa.call(query)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
