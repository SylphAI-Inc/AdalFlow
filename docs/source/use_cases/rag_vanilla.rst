.. raw:: html

   <div style="display: flex; justify-content: flex-start; align-items: center; margin-bottom: 20px;">
      <a href="https://colab.research.google.com/github/SylphAI-Inc/AdalFlow/blob/main/notebooks/tutorials/adalflow_rag_vanilla.ipynb" target="_blank" style="margin-right: 20px;">
         <img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" style="height: 20px;">
      </a>

      <a href="https://github.com/SylphAI-Inc/AdalFlow/tree/main/use_cases/adalflow_rag_vanilla.py" target="_blank" style="display: flex; align-items: center;">
         <img src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png" alt="GitHub" style="height: 20px; width: 20px; margin-right: 5px;">
         <span style="vertical-align: middle;"> Open Source Code</span>
      </a>
   </div>

RAG Vanilla
=============================

Overview
--------

The **RAG Vanilla** implementation is a Retrieval-Augmented Generation pipeline that integrates document
retrieval with natural language generation. This approach allows users to retrieve contextually relevant
documents from a knowledge base and generate informative responses. The code leverages components such as
sentence embeddings, FAISS indexing, and a large language model (LLM) API client.


**Imports**

- **SentenceTransformer**: Used for creating dense vector embeddings for textual data.
- **FAISS**: Provides efficient similarity search using vector indexing.
- **GroqAPIClient and OpenAIClient**: Custom classes for interacting with different LLM providers.
- **ModelType**: Enum for specifying the model type.

.. code-block:: python

    import os
    from typing import List, Dict
    import numpy as np
    from sentence_transformers import SentenceTransformer
    from faiss import IndexFlatL2

    from adalflow.components.model_client import GroqAPIClient, OpenAIClient
    from adalflow.core.types import ModelType
    from adalflow.utils import setup_env


**Pipeline Initialization**

- **Model Client**: Abstracts the API calls to the chosen LLM provider.
- **Embedding Model**: Defaulted to ``all-MiniLM-L6-v2``, generates 384-dimensional embeddings.
- **Vector Dimension**: Dimensionality of the embedding vectors generated by the embedding model ``all-MiniLM-L6-v2``.
- **Top K Retrieval**: Specifies the number of most relevant documents to retrieve.

The ``add_documents()`` function encodes the documents into embeddings and stores them in the FAISS index. It
uses the SentenceTransformer model to generate vector representations of the text. These embeddings are then
added to the FAISS index for efficient similarity search and are also stored in a list for later retrieval.

The ``retrieve_relevant_docs()`` function takes a query string as input and retrieves the top k documents
that are most relevant to the query based on their similarity. The query is first encoded into an embedding,
and then the FAISS index is used to perform a similarity search to identify the documents that are closest
in meaning to the query.

The ``generate_response()`` function constructs a context-aware prompt by combining the retrieved documents
with the user's query. It then calls the model_client to generate a response from the language model. The
conversation history is also maintained, logging each query and its corresponding response for future
reference and context.


.. code-block:: python

    class AdalflowRAGPipeline:
        def __init__(self,
                    model_client = None,
                    model_kwargs = None,
                    embedding_model='all-MiniLM-L6-v2',
                    vector_dim=384,
                    top_k_retrieval=1):
            """
            Initialize RAG Pipeline with embedding and retrieval components

            Args:
                embedding_model (str): Sentence transformer model for embeddings
                vector_dim (int): Dimension of embedding vectors
                top_k_retrieval (int): Number of documents to retrieve
            """
            # Initialize model client for generation
            self.model_client = model_client

            # Initialize embedding model
            self.embedding_model = SentenceTransformer(embedding_model)

            # Initialize FAISS index for vector similarity search
            self.index = IndexFlatL2(vector_dim)

            # Store document texts and their embeddings
            self.documents = []
            self.document_embeddings = []

            # Retrieval parameters
            self.top_k_retrieval = top_k_retrieval

            # Conversation history and context
            self.conversation_history = ""
            self.model_kwargs = model_kwargs

        def add_documents(self, documents: List[str]):
            """
            Add documents to the RAG pipeline's knowledge base

            Args:
                documents (List[str]): List of document texts to add
            """
            for doc in documents:
                # Embed document
                embedding = self.embedding_model.encode(doc)

                # Add to index and document store
                self.index.add(np.array([embedding]))
                self.documents.append(doc)
                self.document_embeddings.append(embedding)

        def retrieve_relevant_docs(self, query: str) -> List[str]:
            """
            Retrieve most relevant documents for a given query

            Args:
                query (str): Input query to find relevant documents

            Returns:
                List[str]: Top k most relevant documents
            """
            # Embed query
            query_embedding = self.embedding_model.encode(query)

            # Perform similarity search
            distances, indices = self.index.search(
                np.array([query_embedding]),
                self.top_k_retrieval
            )

            # Retrieve and return top documents
            return [self.documents[i] for i in indices[0]]

        def generate_response(self, query: str) -> str:
            """
            Generate a response using retrieval-augmented generation

            Args:
                query (str): User's input query

            Returns:
                str: Generated response incorporating retrieved context
            """
            # Retrieve relevant documents
            retrieved_docs = self.retrieve_relevant_docs(query)

            # Construct context-aware prompt
            context = "\n\n".join([f"Context Document: {doc}" for doc in retrieved_docs])
            full_prompt = f"""
            Context:
            {context}

            Query: {query}

            Generate a comprehensive and informative response that:
            1. Uses the provided context documents
            2. Directly answers the query
            3. Incorporates relevant information from the context
            """

            # Prepare API arguments
            api_kwargs = self.model_client.convert_inputs_to_api_kwargs(
                input=full_prompt,
                model_kwargs=self.model_kwargs,
                model_type=ModelType.LLM
            )

            # Call API and parse response
            response = self.model_client.call(
                api_kwargs=api_kwargs,
                model_type=ModelType.LLM
            )
            response_text = self.model_client.parse_chat_completion(response)

            # Update conversation history
            self.conversation_history += f"\nQuery: {query}\nResponse: {response_text}"

            return response_text

**Running the Pipeline**

- **Pipeline Workflow**:
  1. Initializes the ``AdalflowRAGPipeline``.
  2. Adds documents to the knowledge base.
  3. Processes each query to retrieve documents and generate responses.


.. code-block:: python

    def run_rag_pipeline(model_client, model_kwargs, documents, queries):
        rag_pipeline = AdalflowRAGPipeline(model_client=model_client, model_kwargs=model_kwargs)

        rag_pipeline.add_documents(documents)

        # Generate responses
        for query in queries:
            print(f"\nQuery: {query}")
            response = rag_pipeline.generate_response(query)
            print(f"Response: {response}")

- **Documents**: Serve as the knowledge base for validation.
- **Queries**: Designed to test retrieval and generation specific to document content.

.. code-block:: python

    # ajithvcoder's statements are added so that we can validate that the LLM is generating from these lines only
    documents = [
        "ajithvcoder is a good person whom the world knows as Ajith Kumar, ajithvcoder is his nick name that AjithKumar gave himself",
        "The Eiffel Tower is a famous landmark in Paris, built in 1889 for the World's Fair.",
        "ajithvcoder likes Hyderabadi panner dum briyani much.",
        "The Louvre Museum in Paris is the world's largest art museum, housing thousands of works of art.",
        "ajithvcoder has a engineering degree and he graduated on May, 2016."
    ]

    # Questions related to ajithvcoder's are added so that we can validate
    # that the LLM is generating from above given lines only
    queries = [
        "Does Ajith Kumar has any nick name ?",
        "What is the ajithvcoder's favourite food?",
        "When did ajithvcoder graduated ?"
    ]

**API Integration**

- **Generic API Client**: Demonstrates flexibility in using different LLM APIs like Groq and OpenAI without altering the core pipeline logic.

.. code-block:: python

    groq_model_kwargs = {
        "model": "llama-3.2-1b-preview",  # Use 16k model for larger context
        "temperature": 0.1,
        "max_tokens": 800,
    }

    openai_model_kwargs = {
        "model": "gpt-3.5-turbo",  # Use 16k model for larger context
        "temperature": 0.1,
        "max_tokens": 800,
    }

    # Below example shows that adalflow can be used in a genric manner for any api provider
    # without worrying about prompt and parsing results
    model_client = GroqAPIClient()
    run_rag_pipeline(model_client, groq_model_kwargs, documents, queries)
    run_rag_pipeline(OpenAIClient(), openai_model_kwargs, documents, queries)


.. admonition:: API reference
   :class: highlight

   - :class:`utils.setup_env`
   - :class:`core.types.ModelType`
   - :class:`components.model_client.OpenAIClient`
   - :class:`components.model_client.GroqAPIClient`
