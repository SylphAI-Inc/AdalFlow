
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>LLM Evaluation &#8212; LightRAG  documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=292bc364" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=87e54e7c" />
    <link rel="stylesheet" type="text/css" href="../_static/css/rtd_sphinx_search.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../_static/js/rtd_search_config.js"></script>
    <script src="../_static/js/rtd_sphinx_search.min.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'developer_notes/evaluation';</script>
    <link rel="icon" href="../_static/LightRAG-logo-circle.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Parameter" href="parameter.html" />
    <link rel="prev" title="Datasets" href="datasets.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/LightRAG-logo-doc.jpeg" class="logo__image only-light" alt="LightRAG  documentation - Home"/>
    <script>document.write(`<img src="../_static/LightRAG-logo-doc.jpeg" class="logo__image only-dark" alt="LightRAG  documentation - Home"/>`);</script>
  
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../get_started/index.html">
    Get Started
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="index.html">
    Tutorials
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../tutorials/index.html">
    Use Cases
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../apis/index.html">
    API Reference
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
        </div>
      
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/SylphAI-Inc/LightRAG" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discord.gg/ezzszrRZvT" title="Discord" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discord fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discord</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../get_started/index.html">
    Get Started
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="index.html">
    Tutorials
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../tutorials/index.html">
    Use Cases
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../apis/index.html">
    API Reference
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/SylphAI-Inc/LightRAG" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discord.gg/ezzszrRZvT" title="Discord" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discord fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discord</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lightrag_design_philosophy.html">Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="class_hierarchy.html">Class Hierarchy</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Base Classes</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="component.html">Component</a></li>
<li class="toctree-l1"><a class="reference internal" href="base_data_class.html">DataClass</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">RAG Essentials</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="prompt.html">Prompt</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_client.html">ModelClient</a></li>
<li class="toctree-l1"><a class="reference internal" href="generator.html">Generator</a></li>
<li class="toctree-l1"><a class="reference internal" href="output_parsers.html">Parser</a></li>
<li class="toctree-l1"><a class="reference internal" href="embedder.html">Embedder</a></li>
<li class="toctree-l1"><a class="reference internal" href="retriever.html">Retriever</a></li>
<li class="toctree-l1"><a class="reference internal" href="text_splitter.html">Text Splitter</a></li>
<li class="toctree-l1"><a class="reference internal" href="db.html">Data &amp; RAG</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Agent Essentials</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="tool_helper.html">Function calls</a></li>
<li class="toctree-l1"><a class="reference internal" href="agent.html">Agent</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Evaluating</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="datasets.html">Datasets</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">LLM Evaluation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Training</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="parameter.html">Parameter</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimizer.html">Optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="trainer.html">Trainer</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Logging &amp; Tracing &amp; Configurations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging_tracing.html">Logging &amp; Tracing</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="index.html" class="nav-link">Tutorials</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">LLM Evaluation</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="llm-evaluation">
<h1>LLM Evaluation<a class="headerlink" href="#llm-evaluation" title="Link to this heading">#</a></h1>
<div class="highlight admonition">
<p class="admonition-title">Author</p>
<p><a class="reference external" href="https://github.com/mengliu1998">Meng Liu</a></p>
</div>
<p>“You cannot improve what you cannot measure”. This is especially true in the context of LLMs, which have become increasingly popular due to their impressive performance on a wide range of tasks. Evaluating LLMs and their applications is crucial in both research and production to understand their capabilities and limitations.
Overall, such evaluation is a complex and multifaceted process. Below, we provide a guideline for evaluating LLMs and their applications, incorporating aspects outlined by <em>Chang et al.</em> <a class="footnote-reference brackets" href="#id13" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>:</p>
<ul class="simple">
<li><p><strong>What to evaluate</strong>: the tasks and capabilities that LLMs are evaluated on.</p></li>
<li><p><strong>Where to evaluate</strong>: the datasets and benchmarks that are used for evaluation.</p></li>
<li><p><strong>How to evaluate</strong>: the protocols and metrics that are used for evaluation.</p></li>
</ul>
<section id="what-to-evaluate">
<h2>What to evaluate?<a class="headerlink" href="#what-to-evaluate" title="Link to this heading">#</a></h2>
<p>When we are considering the LLM evaluation, the first question that arises is what to evaluate. Deciding what tasks to evaluate or which capabilities to assess is crucial, as it influences both the selection of appropriate benchmarks (where to evaluate) and the choice of evaluation methods (how to evaluate). Below are some commonly evaluated tasks and capabilities of LLMs:</p>
<ul class="simple">
<li><p><em>Natural language understanding</em> (NLU) tasks, such as text classification and sentiment analysis, which evaluate the LLM’s ability to understand natural language.</p></li>
<li><p><em>Natural language generation</em> (NLG) tasks, such as text summarization, translation, and question answering, which evaluate the LLM’s ability to generate natural language.</p></li>
<li><p><em>Reasoning</em> tasks, such as mathematical, logic, and common-sense reasoning, which evaluate the LLM’s ability to perform reasoning and inference to obtain the correct answer.</p></li>
<li><p><em>Robustness</em>, which evaluate the LLM’s ability to generalize to unexpected inputs.</p></li>
<li><p><em>Fairness</em>, which evaluate the LLM’s ability to make unbiased decisions.</p></li>
<li><p><em>Domain adaptation</em>, which evaluate the LLM’s ability to adapt from general language to specific new domains, such as medical or legal texts, coding, etc.</p></li>
<li><p><em>Agent applications</em>, which evaluate the LLM’s ability to use external tools and APIs to perform tasks, such as web search.</p></li>
</ul>
<p>For a more detailed and comprehensive description of the tasks and capabilities that LLMs are evaluated on, please refer to the review papers by <em>Chang et al.</em> <a class="footnote-reference brackets" href="#id13" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> and <em>Guo et al.</em> <a class="footnote-reference brackets" href="#id14" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>.</p>
</section>
<section id="where-to-evaluate">
<h2>Where to evaluate?<a class="headerlink" href="#where-to-evaluate" title="Link to this heading">#</a></h2>
<p>Once we have decided what to evaluate, the next question is where to evaluate. The selection of datasets and benchmarks is important, as it determines the quality and relevance of the evaluation.</p>
<p>To comprehensively assess the capabilities of LLMs, researchers typically utilize benchmarks and datasets that span a broad spectrum of tasks. For example, in the GPT-4 technical report <a class="footnote-reference brackets" href="#id15" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>, the authors employed a variety of general language benchmarks, such as MMLU <a class="footnote-reference brackets" href="#id16" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a>, and academic exams, such as the SAT, GRE, and AP courses, to evaluate the diverse capabilities of GPT-4. Below are some commonly used datasets and benchmarks for evaluating LLMs.</p>
<ul class="simple">
<li><p><em>MMLU</em> <a class="footnote-reference brackets" href="#id16" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a>, which evaluates the LLM’s ability to perform a wide range of language understanding tasks.</p></li>
<li><p><em>HumanEval</em> <a class="footnote-reference brackets" href="#id17" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a>, which measures the LLM’s capability in writing Python code.</p></li>
<li><p><a class="reference external" href="https://crfm.stanford.edu/helm/">HELM</a>, which evaluates LLMs across diverse aspects such as language understanding, generation, common-sense reasoning, and domain adaptation.</p></li>
<li><p><a class="reference external" href="https://arena.lmsys.org/">Chatbot Arena</a>, which is an open platform to evaluate LLMs through human voting.</p></li>
<li><p><a class="reference external" href="https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/api-bank">API-Bank</a>, which evaluates LLMs’ ability to use external tools and APIs to perform tasks.</p></li>
</ul>
<p>Please refer to the review papers (<em>Chang et al.</em> <a class="footnote-reference brackets" href="#id13" id="id8" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>, <em>Guo et al.</em> <a class="footnote-reference brackets" href="#id14" id="id9" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>, and <em>Liu et al.</em> <a class="footnote-reference brackets" href="#id18" id="id10" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a>) for a more comprehensive overview of the datasets and benchmarks used in LLM evaluations. Additionally, a lot of datasets are readily accessible via the <a class="reference external" href="https://huggingface.co/datasets">Hugging Face Datasets</a> library. For instance, the MMLU dataset can be easily loaded from the Hub using the following code snippet.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="linenos">2</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="s2">&quot;cais/mmlu&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;abstract_algebra&#39;</span><span class="p">)</span>
<span class="linenos">3</span><span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">])</span>
<span class="linenos">4</span><span class="c1"># Dataset({</span>
<span class="linenos">5</span><span class="c1"># features: [&#39;question&#39;, &#39;subject&#39;, &#39;choices&#39;, &#39;answer&#39;],</span>
<span class="linenos">6</span><span class="c1"># num_rows: 100</span>
<span class="linenos">7</span><span class="c1"># })</span>
</pre></div>
</div>
</section>
<section id="how-to-evaluate">
<h2>How to evaluate?<a class="headerlink" href="#how-to-evaluate" title="Link to this heading">#</a></h2>
<p>The final question is how to evaluate. Evaluation methods can be divided into <em>automated evaluation</em> and <em>human evaluation</em> (<em>Chang et al.</em> <a class="footnote-reference brackets" href="#id13" id="id11" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> and <em>Liu et al.</em> <a class="footnote-reference brackets" href="#id18" id="id12" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a>). Automated evaluation typically involves using metrics such as accuracy and BERTScore or employing an LLM as the judge, to quantitatively assess the performance of LLMs on specific tasks. Human evaluation, on the other hand, involves human in the loop to evaluate the quality of the generated text or the performance of the LLM. Here, we recommend a few automated evaluation methods that can be used to evaluate LLMs and their applications.</p>
<p>If you are interested in computing metrics such as accuracy, F1-score, ROUGE, BERTScore, perplexity, etc for LLMs and LLM applications, you can check out the metrics provided by <a class="reference external" href="https://huggingface.co/metrics">Hugging Face Metrics</a> or <a class="reference external" href="https://lightning.ai/docs/torchmetrics">TorchMetrics</a>. For instance, to compute the BERTScore, you can use the corresponding metric function provided by Hugging Face, which uses the pre-trained contextual embeddings from BERT and matched words in generated text and reference text by cosine similarity.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_metric</span>
<span class="linenos">2</span><span class="n">bertscore</span> <span class="o">=</span> <span class="n">load_metric</span><span class="p">(</span><span class="s2">&quot;bertscore&quot;</span><span class="p">)</span>
<span class="linenos">3</span><span class="n">generated_text</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;life is good&quot;</span><span class="p">,</span> <span class="s2">&quot;aim for the stars&quot;</span><span class="p">]</span>
<span class="linenos">4</span><span class="n">reference_text</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;life is great&quot;</span><span class="p">,</span> <span class="s2">&quot;make it to the moon&quot;</span><span class="p">]</span>
<span class="linenos">5</span><span class="n">results</span> <span class="o">=</span> <span class="n">bertscore</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">generated_text</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">reference_text</span><span class="p">,</span> <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">)</span>
<span class="linenos">6</span><span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
<span class="linenos">7</span><span class="c1"># {&#39;precision&#39;: [0.9419728517532349, 0.7959791421890259], &#39;recall&#39;: [0.9419728517532349, 0.7749403119087219], &#39;f1&#39;: [0.9419728517532349, 0.7853187918663025], &#39;hashcode&#39;: &#39;distilbert-base-uncased_L5_no-idf_version=0.3.12(hug_trans=4.38.2)&#39;}</span>
</pre></div>
</div>
<p>If you are particulay interested in evaluating RAG (Retrieval-Augmented Generation) pipelines, we have several metrics available in LightRAG to assess both the quality of the retrieved context and the quality of the final generated answer.</p>
<ul class="simple">
<li><p><a class="reference internal" href="../apis/eval/eval.retriever_recall.html#module-eval.retriever_recall" title="eval.retriever_recall"><code class="xref py py-class docutils literal notranslate"><span class="pre">RetrieverRecall</span></code></a>: This is used to evaluate the recall of the retriever component of the RAG pipeline.</p></li>
<li><p><a class="reference internal" href="../apis/eval/eval.retriever_relevance.html#module-eval.retriever_relevance" title="eval.retriever_relevance"><code class="xref py py-class docutils literal notranslate"><span class="pre">RetrieverRelevance</span></code></a>: This is used to evaluate the relevance of the retrieved context to the query.</p></li>
<li><p><a class="reference internal" href="../apis/eval/eval.answer_match_acc.html#module-eval.answer_match_acc" title="eval.answer_match_acc"><code class="xref py py-class docutils literal notranslate"><span class="pre">AnswerMatchAcc</span></code></a>: This calculates the exact match accuracy or fuzzy match accuracy of the generated answers by comparing them to the ground truth answers.</p></li>
<li><p><a class="reference internal" href="../apis/eval/eval.llm_as_judge.html#module-eval.llm_as_judge" title="eval.llm_as_judge"><code class="xref py py-class docutils literal notranslate"><span class="pre">LLMasJudge</span></code></a>: This uses an LLM to get the judgement of the generated answer for a list of questions. The task description and the judgement query of the LLM judge can be customized. It computes the judgement score, which is the number of generated answers that are judged as correct by the LLM divided by the total number of generated answers.</p></li>
</ul>
<p>For example, you can use the following code snippet to compute the recall and relevance of the retriever component of the RAG pipeline for a single query.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="kn">from</span> <span class="nn">lightrag.eval</span> <span class="kn">import</span> <span class="n">RetrieverRecall</span><span class="p">,</span> <span class="n">RetrieverRelevance</span>
<span class="linenos"> 2</span><span class="n">retrieved_contexts</span> <span class="o">=</span> <span class="p">[</span>
<span class="linenos"> 3</span>    <span class="s2">&quot;Apple is founded before Google.&quot;</span><span class="p">,</span>
<span class="linenos"> 4</span>    <span class="s2">&quot;Feburary has 28 days in common years. Feburary has 29 days in leap years. Feburary is the second month of the year.&quot;</span><span class="p">,</span>
<span class="linenos"> 5</span><span class="p">]</span>
<span class="linenos"> 6</span><span class="n">gt_contexts</span> <span class="o">=</span> <span class="p">[</span>
<span class="linenos"> 7</span>    <span class="p">[</span>
<span class="linenos"> 8</span>        <span class="s2">&quot;Apple is founded in 1976.&quot;</span><span class="p">,</span>
<span class="linenos"> 9</span>        <span class="s2">&quot;Google is founded in 1998.&quot;</span><span class="p">,</span>
<span class="linenos">10</span>        <span class="s2">&quot;Apple is founded before Google.&quot;</span><span class="p">,</span>
<span class="linenos">11</span>    <span class="p">],</span>
<span class="linenos">12</span>    <span class="p">[</span><span class="s2">&quot;Feburary has 28 days in common years&quot;</span><span class="p">,</span> <span class="s2">&quot;Feburary has 29 days in leap years&quot;</span><span class="p">],</span>
<span class="linenos">13</span><span class="p">]</span>
<span class="linenos">14</span><span class="n">retriever_recall</span> <span class="o">=</span> <span class="n">RetrieverRecall</span><span class="p">()</span>
<span class="linenos">15</span><span class="n">avg_recall</span><span class="p">,</span> <span class="n">recall_list</span> <span class="o">=</span> <span class="n">retriever_recall</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">retrieved_contexts</span><span class="p">,</span> <span class="n">gt_contexts</span><span class="p">)</span> <span class="c1"># Compute the recall of the retriever</span>
<span class="linenos">16</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Recall: </span><span class="si">{</span><span class="n">avg_recall</span><span class="si">}</span><span class="s2">, Recall List: </span><span class="si">{</span><span class="n">recall_list</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="linenos">17</span><span class="c1"># Recall: 0.6666666666666666, Recall List: [0.3333333333333333, 1.0]</span>
<span class="linenos">18</span><span class="n">retriever_relevance</span> <span class="o">=</span> <span class="n">RetrieverRelevance</span><span class="p">()</span>
<span class="linenos">19</span><span class="n">avg_relevance</span><span class="p">,</span> <span class="n">relevance_list</span> <span class="o">=</span> <span class="n">retriever_relevance</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">retrieved_contexts</span><span class="p">,</span> <span class="n">gt_contexts</span><span class="p">)</span> <span class="c1"># Compute the relevance of the retriever</span>
<span class="linenos">20</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Relevance: </span><span class="si">{</span><span class="n">avg_relevance</span><span class="si">}</span><span class="s2">, Relevance List: </span><span class="si">{</span><span class="n">relevance_list</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="linenos">21</span><span class="c1"># Relevance: 0.803030303030303, Relevance List: [1.0, 0.6060606060606061]</span>
</pre></div>
</div>
<p>For a more detailed instructions on how build and evaluate RAG pipelines, you can refer to the use case on <a class="reference internal" href="../tutorials/eval_a_rag.html"><span class="doc">Evaluating a RAG Pipeline</span></a>.</p>
<p>If you intent to use metrics that are not available in the LightRAG library, you can also implement your own custom metric functions or use other libraries such as <a class="reference external" href="https://docs.ragas.io/en/stable/getstarted/index.html">RAGAS</a> to compute the desired metrics for evaluating RAG pipelines.</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id13" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id2">2</a>,<a role="doc-backlink" href="#id8">3</a>,<a role="doc-backlink" href="#id11">4</a>)</span>
<p>Chang, Yupeng, et al. “A survey on evaluation of large language models.” ACM Transactions on Intelligent Systems and Technology 15.3 (2024): 1-45.</p>
</aside>
<aside class="footnote brackets" id="id14" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id3">1</a>,<a role="doc-backlink" href="#id9">2</a>)</span>
<p>Guo, Zishan, et al. “Evaluating large language models: A comprehensive survey.” arXiv preprint arXiv:2310.19736 (2023).</p>
</aside>
<aside class="footnote brackets" id="id15" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">3</a><span class="fn-bracket">]</span></span>
<p>Achiam, Josh, et al. “GPT-4 technical report.” arXiv preprint arXiv:2303.08774 (2023).</p>
</aside>
<aside class="footnote brackets" id="id16" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id5">1</a>,<a role="doc-backlink" href="#id6">2</a>)</span>
<p>Hendrycks, Dan, et al. “Measuring massive multitask language understanding.” International Conference on Learning Representations. 2020.</p>
</aside>
<aside class="footnote brackets" id="id17" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">5</a><span class="fn-bracket">]</span></span>
<p>Chen, Mark, et al. “Evaluating large language models trained on code.” arXiv preprint arXiv:2107.03374 (2021).</p>
</aside>
<aside class="footnote brackets" id="id18" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id10">1</a>,<a role="doc-backlink" href="#id12">2</a>)</span>
<p>Liu, Yang, et al. “Datasets for Large Language Models: A Comprehensive Survey.” arXiv preprint arXiv:2402.18041 (2024).</p>
</aside>
</aside>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="datasets.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Datasets</p>
      </div>
    </a>
    <a class="right-next"
       href="parameter.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Parameter</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-to-evaluate">What to evaluate?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#where-to-evaluate">Where to evaluate?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-evaluate">How to evaluate?</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2024, SylphAI, Inc.
      <br/>
    
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>