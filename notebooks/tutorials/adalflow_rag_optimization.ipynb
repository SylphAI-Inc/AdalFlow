{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHF95Kr4CzGq"
   },
   "source": [
    "# ü§ó Welcome to AdalFlow!\n",
    "## The PyTorch library to auto-optimize any LLM task pipelines\n",
    "\n",
    "Thanks for trying us out, we're here to provide you with the best LLM application development experience you can dream of üòä any questions or concerns you may have, [come talk to us on discord,](https://discord.gg/ezzszrRZvT) we're always here to help! ‚≠ê <i>Star us on <a href=\"https://github.com/SylphAI-Inc/AdalFlow\">Github</a> </i> ‚≠ê\n",
    "\n",
    "\n",
    "# Quick Links\n",
    "\n",
    "Github repo: https://github.com/SylphAI-Inc/AdalFlow\n",
    "\n",
    "Full Tutorials: https://adalflow.sylph.ai/index.html#.\n",
    "\n",
    "Deep dive on each API: check out the [developer notes](https://adalflow.sylph.ai/tutorials/index.html).\n",
    "\n",
    "Common use cases along with the auto-optimization:  check out [Use cases](https://adalflow.sylph.ai/use_cases/index.html).\n",
    "\n",
    "## üìñ Outline\n",
    "\n",
    "In this tutorial, we will cover the auto-optimization of a standard RAG:\n",
    "\n",
    "- Introducing HotPotQA dataset and HotPotQAData class.\n",
    "\n",
    "- Convert Dspy‚Äôs Retriever to AdalFlow‚Äôs Retriever to easy comparison.\n",
    "\n",
    "- Build the standard RAG with Retriever and Generator components.\n",
    "\n",
    "- Learn how to connect the output-input between components to enable auto-text-grad optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kof5M6DRaKhh"
   },
   "source": [
    "\n",
    "# Installation\n",
    "\n",
    "1. Use `pip` to install the `adalflow` Python package. We will need `openai`, `groq` from the extra packages.\n",
    "\n",
    "  ```bash\n",
    "  pip install adalflow[openai,groq]\n",
    "  ```\n",
    "2. Setup  `openai` and `groq` API key in the environment variables\n",
    "\n",
    "You can choose to use different client. You can import the model client you prefer. We support `Anthropic`, `Cohere`, `Google`, `GROQ`, `OpenAI`, `Transformer` and more in development. We will use OpenAI here as an example.Please refer to our [full installation guide](https://adalflow.sylph.ai/get_started/installation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tAp3eDjOCma1"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!pip install -U adalflow[openai] # also install the package for the model client you'll use\n",
    "!pip install dspy\n",
    "!pip install datasets\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: httpx 0.28.1\n",
      "Uninstalling httpx-0.28.1:\n",
      "  Successfully uninstalled httpx-0.28.1\n",
      "Found existing installation: anyio 4.9.0\n",
      "Uninstalling anyio-4.9.0:\n",
      "  Successfully uninstalled anyio-4.9.0\n",
      "\u001b[33mDEPRECATION: Loading egg at /Users/jinhakim/miniforge3/lib/python3.12/site-packages/matlabengine-24.2-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting anyio<4.0,>=3.1.0\n",
      "  Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/jinhakim/miniforge3/lib/python3.12/site-packages (from anyio<4.0,>=3.1.0) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/jinhakim/miniforge3/lib/python3.12/site-packages (from anyio<4.0,>=3.1.0) (1.3.1)\n",
      "Downloading anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "Installing collected packages: anyio\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mcp 1.9.4 requires httpx>=0.27, which is not installed.\n",
      "openai 1.88.0 requires httpx<1,>=0.23.0, which is not installed.\n",
      "litellm 1.75.3 requires httpx>=0.23.0, which is not installed.\n",
      "fastmcp 2.9.0 requires httpx>=0.28.1, which is not installed.\n",
      "mcp 1.9.4 requires anyio>=4.5, but you have anyio 3.7.1 which is incompatible.\n",
      "sse-starlette 2.3.6 requires anyio>=4.7.0, but you have anyio 3.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed anyio-3.7.1\n",
      "\u001b[33mDEPRECATION: Loading egg at /Users/jinhakim/miniforge3/lib/python3.12/site-packages/matlabengine-24.2-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting httpx==0.24.1\n",
      "  Downloading httpx-0.24.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: certifi in /Users/jinhakim/miniforge3/lib/python3.12/site-packages (from httpx==0.24.1) (2025.6.15)\n",
      "Collecting httpcore<0.18.0,>=0.15.0 (from httpx==0.24.1)\n",
      "  Downloading httpcore-0.17.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: idna in /Users/jinhakim/miniforge3/lib/python3.12/site-packages (from httpx==0.24.1) (3.10)\n",
      "Requirement already satisfied: sniffio in /Users/jinhakim/miniforge3/lib/python3.12/site-packages (from httpx==0.24.1) (1.3.1)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore<0.18.0,>=0.15.0->httpx==0.24.1)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /Users/jinhakim/miniforge3/lib/python3.12/site-packages (from httpcore<0.18.0,>=0.15.0->httpx==0.24.1) (3.7.1)\n",
      "Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
      "Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: h11, httpcore, httpx\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.16.0\n",
      "    Uninstalling h11-0.16.0:\n",
      "      Successfully uninstalled h11-0.16.0\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.9\n",
      "    Uninstalling httpcore-1.0.9:\n",
      "      Successfully uninstalled httpcore-1.0.9\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mcp 1.9.4 requires anyio>=4.5, but you have anyio 3.7.1 which is incompatible.\n",
      "mcp 1.9.4 requires httpx>=0.27, but you have httpx 0.24.1 which is incompatible.\n",
      "python-telegram-bot 21.10 requires httpx~=0.27, but you have httpx 0.24.1 which is incompatible.\n",
      "fastmcp 2.9.0 requires httpx>=0.28.1, but you have httpx 0.24.1 which is incompatible.\n",
      "langgraph-sdk 0.1.70 requires httpx>=0.25.2, but you have httpx 0.24.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-0.17.3 httpx-0.24.1\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall httpx anyio -y\n",
    "!pip install \"anyio>=3.1.0,<4.0\"\n",
    "!pip install httpx==0.24.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KapUyHMM07pJ"
   },
   "source": [
    "## Set Environment Variables\n",
    "\n",
    "Run the following code and pass your api key.\n",
    "\n",
    "Note: for normal `.py` projects, follow our [official installation guide](https://lightrag.sylph.ai/get_started/installation.html).\n",
    "\n",
    "*Go to [OpenAI](https://platform.openai.com/docs/introduction) to get API keys if you don't already have.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ONfzF9Puzdd_",
    "outputId": "5fc0cd30-9ae7-443a-c06c-31e9edeafd69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API keys have been set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "# Prompt user to enter their API keys securely\n",
    "openai_api_key = getpass(\"Please enter your OpenAI API key: \")\n",
    "\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "print(\"API keys have been set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aE3I05BqOmd7"
   },
   "outputs": [],
   "source": [
    "import dspy\n",
    "import re\n",
    "from typing import List, Union, Optional, Dict, Callable, Any, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "import adalflow as adal\n",
    "from adalflow.optim.parameter import Parameter, ParameterType\n",
    "from adalflow.datasets.hotpot_qa import HotPotQA, HotPotQAData\n",
    "from adalflow.datasets.types import Example\n",
    "from adalflow.core.types import RetrieverOutput\n",
    "from adalflow.core import Component, Generator\n",
    "from adalflow.core.retriever import Retriever\n",
    "from adalflow.core.component import func_to_data_component\n",
    "from adalflow.components.model_client.openai_client import OpenAIClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cqUUoua9fUxQ"
   },
   "outputs": [],
   "source": [
    "gpt_4o_model = {\n",
    "    \"model_client\": OpenAIClient(),\n",
    "    \"model_kwargs\": {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"max_tokens\": 2000,\n",
    "    },\n",
    "}\n",
    "\n",
    "gpt_3_model = {\n",
    "    \"model_client\": OpenAIClient(),\n",
    "    \"model_kwargs\": {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"max_tokens\": 2000,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0irHeHUkOmL8",
    "outputId": "61f778a2-9ec1-4fda-daa2-bcc7f31baa78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_csv_path: /Users/jinhakim/.adalflow/cache_datasets/hotpot_qa_dev_titles/train.json\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dataset scripts are no longer supported, but found hotpot_qa.py",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     15\u001b[39m     answer: \u001b[38;5;28mstr\u001b[39m = field(\n\u001b[32m     16\u001b[39m         metadata={\u001b[33m\"\u001b[39m\u001b[33mdesc\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mThe answer you produced\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m     17\u001b[39m     )\n\u001b[32m     19\u001b[39m     __output_fields__ = [\u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m dataset = \u001b[43mHotPotQA\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(dataset[\u001b[32m0\u001b[39m], \u001b[38;5;28mtype\u001b[39m(dataset[\u001b[32m0\u001b[39m]))\n\u001b[32m     25\u001b[39m HotPotQAData(\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mid\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33m5a8b57f25542995d1e6f1371\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m     question=\u001b[33m\"\u001b[39m\u001b[33mWere Scott Derrickson and Ed Wood of the same nationality?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     28\u001b[39m     answer=\u001b[33m\"\u001b[39m\u001b[33myes\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     29\u001b[39m     gold_titles=\u001b[33m\"\u001b[39m\u001b[33m{\u001b[39m\u001b[33m'\u001b[39m\u001b[33mScott Derrickson\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mEd Wood\u001b[39m\u001b[33m'\u001b[39m\u001b[33m}\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     30\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/AdalFlow/adalflow/adalflow/datasets/hotpot_qa.py:50\u001b[39m, in \u001b[36mHotPotQA.__init__\u001b[39m\u001b[34m(self, only_hard_examples, root, split, keep_details, size, **kwargs)\u001b[39m\n\u001b[32m     48\u001b[39m split_csv_path = os.path.join(data_path, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msplit_csv_path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_csv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_or_download_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_csv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monly_hard_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_details\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# load from csv\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;28mself\u001b[39m.data = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/AdalFlow/adalflow/adalflow/datasets/hotpot_qa.py:111\u001b[39m, in \u001b[36mHotPotQA._check_or_download_dataset\u001b[39m\u001b[34m(self, data_path, split, only_hard_examples, keep_details)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m only_hard_examples, (\n\u001b[32m    101\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mCare must be taken when adding support for easy examples.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    102\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDev must be all hard to match official dev, but training can be flexible.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    103\u001b[39m )\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# hf_official_train = load_dataset(\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m#     \"hotpot_qa\", \"fullwiki\", split=\"train\", trust_remote_code=True\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# hf_official_dev = load_dataset(\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m#     \"hotpot_qa\", \"fullwiki\", split=\"validation\", trust_remote_code=True\u001b[39;00m\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m hf_official_train = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhotpot_qa\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfullwiki\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    113\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m hf_official_dev = load_dataset(\n\u001b[32m    115\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhotpot_qa\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfullwiki\u001b[39m\u001b[33m\"\u001b[39m, split=\u001b[33m\"\u001b[39m\u001b[33mvalidation\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    116\u001b[39m )\n\u001b[32m    117\u001b[39m data_path_dir = os.path.dirname(data_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/adalflow-project-Y74mvs4e-py3.12/lib/python3.12/site-packages/datasets/load.py:1392\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1387\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   1388\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   1389\u001b[39m )\n\u001b[32m   1391\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1392\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1395\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   1408\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/adalflow-project-Y74mvs4e-py3.12/lib/python3.12/site-packages/datasets/load.py:1132\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1131\u001b[39m     features = _fix_for_backward_compatible_features(features)\n\u001b[32m-> \u001b[39m\u001b[32m1132\u001b[39m dataset_module = \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n\u001b[32m   1142\u001b[39m builder_kwargs = dataset_module.builder_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/adalflow-project-Y74mvs4e-py3.12/lib/python3.12/site-packages/datasets/load.py:1031\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[39m\n\u001b[32m   1026\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[32m   1027\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m   1028\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1029\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1030\u001b[39m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1031\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1032\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1033\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/adalflow-project-Y74mvs4e-py3.12/lib/python3.12/site-packages/datasets/load.py:989\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    982\u001b[39m     api.hf_hub_download(\n\u001b[32m    983\u001b[39m         repo_id=path,\n\u001b[32m    984\u001b[39m         filename=filename,\n\u001b[32m   (...)\u001b[39m\u001b[32m    987\u001b[39m         proxies=download_config.proxies,\n\u001b[32m    988\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset scripts are no longer supported, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    990\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n\u001b[32m    991\u001b[39m     \u001b[38;5;66;03m# Use the infos from the parquet export except in some cases:\u001b[39;00m\n\u001b[32m    992\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data_dir \u001b[38;5;129;01mor\u001b[39;00m data_files \u001b[38;5;129;01mor\u001b[39;00m (revision \u001b[38;5;129;01mand\u001b[39;00m revision != \u001b[33m\"\u001b[39m\u001b[33mmain\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mRuntimeError\u001b[39m: Dataset scripts are no longer supported, but found hotpot_qa.py"
     ]
    }
   ],
   "source": [
    "def load_datasets():\n",
    "\n",
    "    trainset = HotPotQA(split=\"train\", size=20)\n",
    "    valset = HotPotQA(split=\"val\", size=50)\n",
    "    testset = HotPotQA(split=\"test\", size=50)\n",
    "    print(f\"trainset, valset: {len(trainset)}, {len(valset)}, example: {trainset[0]}\")\n",
    "    return trainset, valset, testset\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AnswerData(adal.DataClass):\n",
    "    reasoning: str = field(\n",
    "        metadata={\"desc\": \"The reasoning to produce the answer\"},\n",
    "    )\n",
    "    answer: str = field(\n",
    "        metadata={\"desc\": \"The answer you produced\"},\n",
    "    )\n",
    "\n",
    "    __output_fields__ = [\"reasoning\", \"answer\"]\n",
    "\n",
    "\n",
    "dataset = HotPotQA(split=\"train\", size=20)\n",
    "print(dataset[0], type(dataset[0]))\n",
    "\n",
    "HotPotQAData(\n",
    "    id=\"5a8b57f25542995d1e6f1371\",\n",
    "    question=\"Were Scott Derrickson and Ed Wood of the same nationality?\",\n",
    "    answer=\"yes\",\n",
    "    gold_titles=\"{'Scott Derrickson', 'Ed Wood'}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ZZIEtZYHNVjo"
   },
   "outputs": [],
   "source": [
    "class DspyRetriever(adal.Retriever):\n",
    "    def __init__(self, top_k: int = 3):\n",
    "        super().__init__()\n",
    "        self.top_k = top_k\n",
    "        self.dspy_retriever = dspy.Retrieve(k=top_k)\n",
    "\n",
    "    def call(\n",
    "        self, input: str, top_k: Optional[int] = None\n",
    "    ) -> List[adal.RetrieverOutput]:\n",
    "\n",
    "        k = top_k or self.top_k\n",
    "\n",
    "        output = self.dspy_retriever(query_or_queries=input, k=k)\n",
    "        final_output: List[RetrieverOutput] = []\n",
    "        documents = output.passages\n",
    "\n",
    "        final_output.append(\n",
    "            RetrieverOutput(\n",
    "                query=input,\n",
    "                documents=documents,\n",
    "                doc_indices=[],\n",
    "            )\n",
    "        )\n",
    "        return final_output\n",
    "\n",
    "\n",
    "def test_retriever():\n",
    "    question = \"How many storeys are in the castle that David Gregory inherited?\"\n",
    "    retriever = DspyRetriever(top_k=3)\n",
    "    retriever_out = retriever(input=question)\n",
    "    print(f\"retriever_out: {retriever_out}\")\n",
    "\n",
    "\n",
    "def call(\n",
    "    self, question: str, id: Optional[str] = None\n",
    ") -> Union[adal.GeneratorOutput, adal.Parameter]:\n",
    "    prompt_kwargs = self._prepare_input(question)\n",
    "    output = self.llm(prompt_kwargs=prompt_kwargs, id=id)\n",
    "    return output\n",
    "\n",
    "\n",
    "def call(self, question: str, id: str = None) -> adal.GeneratorOutput:\n",
    "    if self.training:\n",
    "        raise ValueError(\"This component is not supposed to be called in training mode\")\n",
    "\n",
    "    retriever_out = self.retriever.call(input=question)\n",
    "\n",
    "    successor_map_fn = lambda x: (  # noqa E731\n",
    "        \"\\n\\n\".join(x[0].documents) if x and x[0] and x[0].documents else \"\"\n",
    "    )\n",
    "    retrieved_context = successor_map_fn(retriever_out)\n",
    "\n",
    "    prompt_kwargs = {\n",
    "        \"context\": retrieved_context,\n",
    "        \"question\": question,\n",
    "    }\n",
    "\n",
    "    output = self.llm.call(\n",
    "        prompt_kwargs=prompt_kwargs,\n",
    "        id=id,\n",
    "    )\n",
    "    return output\n",
    "\n",
    "\n",
    "def forward(self, question: str, id: str = None) -> adal.Parameter:\n",
    "    if not self.training:\n",
    "        raise ValueError(\"This component is not supposed to be called in eval mode\")\n",
    "    retriever_out = self.retriever.forward(input=question)\n",
    "    successor_map_fn = lambda x: (  # noqa E731\n",
    "        \"\\n\\n\".join(x.data[0].documents)\n",
    "        if x.data and x.data[0] and x.data[0].documents\n",
    "        else \"\"\n",
    "    )\n",
    "    retriever_out.add_successor_map_fn(successor=self.llm, map_fn=successor_map_fn)\n",
    "    generator_out = self.llm.forward(\n",
    "        prompt_kwargs={\"question\": question, \"context\": retriever_out}, id=id\n",
    "    )\n",
    "    return generator_out\n",
    "\n",
    "\n",
    "def bicall(\n",
    "    self, question: str, id: str = None\n",
    ") -> Union[adal.GeneratorOutput, adal.Parameter]:\n",
    "    \"\"\"You can also combine both the forward and call in the same function.\n",
    "    Supports both training and eval mode by using __call__ for GradComponents\n",
    "    like Retriever and Generator\n",
    "    \"\"\"\n",
    "    retriever_out = self.retriever(input=question)\n",
    "    if isinstance(retriever_out, adal.Parameter):\n",
    "        successor_map_fn = lambda x: (  # noqa E731\n",
    "            \"\\n\\n\".join(x.data[0].documents)\n",
    "            if x.data and x.data[0] and x.data[0].documents\n",
    "            else \"\"\n",
    "        )\n",
    "        retriever_out.add_successor_map_fn(successor=self.llm, map_fn=successor_map_fn)\n",
    "    else:\n",
    "        successor_map_fn = lambda x: (  # noqa E731\n",
    "            \"\\n\\n\".join(x[0].documents) if x and x[0] and x[0].documents else \"\"\n",
    "        )\n",
    "        retrieved_context = successor_map_fn(retriever_out)\n",
    "    prompt_kwargs = {\n",
    "        \"context\": retrieved_context,\n",
    "        \"question\": question,\n",
    "    }\n",
    "    output = self.llm(prompt_kwargs=prompt_kwargs, id=id)\n",
    "    return output\n",
    "\n",
    "\n",
    "task_desc_str = r\"\"\"Answer questions with short factoid answers.\n",
    "\n",
    "You will receive context(may contain relevant facts) and a question.\n",
    "Think step by step.\"\"\"\n",
    "\n",
    "\n",
    "class VanillaRAG(adal.GradComponent):\n",
    "    def __init__(self, passages_per_hop=3, model_client=None, model_kwargs=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.passages_per_hop = passages_per_hop\n",
    "\n",
    "        self.retriever = DspyRetriever(top_k=passages_per_hop)\n",
    "        self.llm_parser = adal.DataClassParser(\n",
    "            data_class=AnswerData, return_data_class=True, format_type=\"json\"\n",
    "        )\n",
    "        self.llm = Generator(\n",
    "            model_client=model_client,\n",
    "            model_kwargs=model_kwargs,\n",
    "            prompt_kwargs={\n",
    "                \"task_desc_str\": adal.Parameter(\n",
    "                    data=task_desc_str,\n",
    "                    role_desc=\"Task description for the language model\",\n",
    "                    param_type=adal.ParameterType.PROMPT,\n",
    "                ),\n",
    "                \"few_shot_demos\": adal.Parameter(\n",
    "                    data=None,\n",
    "                    requires_opt=True,\n",
    "                    role_desc=\"To provide few shot demos to the language model\",\n",
    "                    param_type=adal.ParameterType.DEMOS,\n",
    "                ),\n",
    "                \"output_format_str\": self.llm_parser.get_output_format_str(),\n",
    "            },\n",
    "            template=answer_template,\n",
    "            output_processors=self.llm_parser,\n",
    "            use_cache=True,\n",
    "        )\n",
    "\n",
    "\n",
    "class VallinaRAGAdal(adal.AdalComponent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_client: adal.ModelClient,\n",
    "        model_kwargs: Dict,\n",
    "        backward_engine_model_config: Dict | None = None,\n",
    "        teacher_model_config: Dict | None = None,\n",
    "        text_optimizer_model_config: Dict | None = None,\n",
    "    ):\n",
    "        task = VanillaRAG(\n",
    "            model_client=model_client,\n",
    "            model_kwargs=model_kwargs,\n",
    "            passages_per_hop=3,\n",
    "        )\n",
    "        eval_fn = AnswerMatchAcc(type=\"fuzzy_match\").compute_single_item\n",
    "        loss_fn = adal.EvalFnToTextLoss(\n",
    "            eval_fn=eval_fn, eval_fn_desc=\"fuzzy_match: 1 if str(y) in str(y_gt) else 0\"\n",
    "        )\n",
    "        super().__init__(\n",
    "            task=task,\n",
    "            eval_fn=eval_fn,\n",
    "            loss_fn=loss_fn,\n",
    "            backward_engine_model_config=backward_engine_model_config,\n",
    "            teacher_model_config=teacher_model_config,\n",
    "            text_optimizer_model_config=text_optimizer_model_config,\n",
    "        )\n",
    "\n",
    "    # tell the trainer how to call the task\n",
    "    def prepare_task(self, sample: HotPotQAData) -> Tuple[Callable[..., Any], Dict]:\n",
    "        if self.task.training:\n",
    "            return self.task.forward, {\"question\": sample.question, \"id\": sample.id}\n",
    "        else:\n",
    "            return self.task.call, {\"question\": sample.question, \"id\": sample.id}\n",
    "\n",
    "    # eval mode: get the generator output, directly engage with the eval_fn\n",
    "    def prepare_eval(self, sample: HotPotQAData, y_pred: adal.GeneratorOutput) -> float:\n",
    "        y_label = \"\"\n",
    "        if y_pred and y_pred.data and y_pred.data.answer:\n",
    "            y_label = y_pred.data.answer\n",
    "        return self.eval_fn, {\"y\": y_label, \"y_gt\": sample.answer}\n",
    "\n",
    "    # train mode: get the loss and get the data from the full_response\n",
    "    def prepare_loss(self, sample: HotPotQAData, pred: adal.Parameter):\n",
    "        # prepare gt parameter\n",
    "        y_gt = adal.Parameter(\n",
    "            name=\"y_gt\",\n",
    "            data=sample.answer,\n",
    "            eval_input=sample.answer,\n",
    "            requires_opt=False,\n",
    "        )\n",
    "\n",
    "        # pred's full_response is the output of the task pipeline which is GeneratorOutput\n",
    "        pred.eval_input = (\n",
    "            pred.full_response.data.answer\n",
    "            if pred.full_response\n",
    "            and pred.full_response.data\n",
    "            and pred.full_response.data.answer\n",
    "            else \"\"\n",
    "        )\n",
    "        return self.loss_fn, {\"kwargs\": {\"y\": pred, \"y_gt\": y_gt}}\n",
    "\n",
    "\n",
    "def train_diagnose(\n",
    "    model_client: adal.ModelClient,\n",
    "    model_kwargs: Dict,\n",
    ") -> Dict:\n",
    "\n",
    "    trainset, valset, testset = load_datasets()\n",
    "\n",
    "    adal_component = VallinaRAGAdal(\n",
    "        model_client,\n",
    "        model_kwargs,\n",
    "        backward_engine_model_config=gpt_4o_model,\n",
    "        teacher_model_config=gpt_3_model,\n",
    "        text_optimizer_model_config=gpt_3_model,\n",
    "    )\n",
    "    trainer = adal.Trainer(adaltask=adal_component)\n",
    "    trainer.diagnose(dataset=trainset, split=\"train\")\n",
    "    # trainer.diagnose(dataset=valset, split=\"val\")\n",
    "    # trainer.diagnose(dataset=testset, split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmkbyxmuruUu"
   },
   "source": [
    "# Issues and feedback\n",
    "\n",
    "If you encounter any issues, please report them here: [GitHub Issues](https://github.com/SylphAI-Inc/LightRAG/issues).\n",
    "\n",
    "For feedback, you can use either the [GitHub discussions](https://github.com/SylphAI-Inc/LightRAG/discussions) or [Discord](https://discord.gg/ezzszrRZvT)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "adalflow-project-Y74mvs4e-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
