{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: flex-start; align-items: center; gap: 15px; margin-bottom: 20px;\">\n",
    "  <a target=\"_blank\" href=\"https://colab.research.google.com/github.com/SylphAI-Inc/AdalFlow/blob/main/notebooks/tutorials/adalflow_rag_documents.ipynb\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "  </a>\n",
    "  <a href=\"https://github.com/SylphAI-Inc/AdalFlow/blob/main/tutorials/adalflow_rag_documents.py\" target=\"_blank\" style=\"display: flex; align-items: center;\">\n",
    "      <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" alt=\"GitHub\" style=\"height: 20px; width: 20px; margin-right: 5px;\">\n",
    "      <span style=\"vertical-align: middle;\"> Open Source Code </span>\n",
    "  </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ó Welcome to AdalFlow!\n",
    "## The PyTorch library to auto-optimize any LLM task pipelines\n",
    "\n",
    "Thanks for trying us out, we're here to provide you with the best LLM application development experience you can dream of üòä any questions or concerns you may have, [come talk to us on discord,](https://discord.gg/ezzszrRZvT) we're always here to help! ‚≠ê <i>Star us on <a href=\"https://github.com/SylphAI-Inc/AdalFlow\">Github</a> </i> ‚≠ê\n",
    "\n",
    "\n",
    "# Quick Links\n",
    "\n",
    "Github repo: https://github.com/SylphAI-Inc/AdalFlow\n",
    "\n",
    "Full Tutorials: https://adalflow.sylph.ai/index.html#.\n",
    "\n",
    "Deep dive on each API: check out the [developer notes](https://adalflow.sylph.ai/tutorials/index.html).\n",
    "\n",
    "Common use cases along with the auto-optimization:  check out [Use cases](https://adalflow.sylph.ai/use_cases/index.html).\n",
    "\n",
    "# Author\n",
    "This notebook was created by community contributor [Ajith](https://github.com/ajithvcoder/).\n",
    "\n",
    "# Outline\n",
    "\n",
    "This is a quick introduction of what AdalFlow is capable of. We will cover:\n",
    "\n",
    "* How to use adalflow for rag with documents\n",
    "\n",
    "Adalflow can be used in a genric manner for any api provider without worrying much about prompt, \n",
    "model args and parsing results\n",
    "\n",
    "**Next: Try our [adalflow-text-splitter](\"https://colab.research.google.com/github.com/SylphAI-Inc/AdalFlow/blob/main/notebooks/tutorials/adalflow_text_splitter.ipynb\")**\n",
    "\n",
    "\n",
    "# Installation\n",
    "\n",
    "1. Use `pip` to install the `adalflow` Python package. We will need `openai`, `groq`, and `faiss`(cpu version) from the extra packages.\n",
    "\n",
    "  ```bash\n",
    "    pip install torch --index-url https://download.pytorch.org/whl/cpu\n",
    "    pip install sentence-transformers==3.3.1\n",
    "    pip install adalflow[openai,groq,faiss-cpu]\n",
    "  ```\n",
    "2. Setup  `openai` and `groq` API key in the environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Environment Variables\n",
    "\n",
    "Note: Enter your api keys in below cell #todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .env\n"
     ]
    }
   ],
   "source": [
    "%%writefile .env\n",
    "\n",
    "OPENAI_API_KEY=\"PASTE-OPENAI_API_KEY_HERE\"\n",
    "GROQ_API_KEY=\"PASTE-GROQ_API_KEY-HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adalflow.utils import setup_env\n",
    "\n",
    "# Load environment variables - Make sure to have OPENAI_API_KEY in .env file and .env is present in current folder\n",
    "setup_env(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/ajithdev/AdalFlow/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from faiss import IndexFlatL2\n",
    "\n",
    "from adalflow.components.model_client import GroqAPIClient, OpenAIClient\n",
    "from adalflow.core.types import ModelType\n",
    "from adalflow.utils import setup_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AdalflowRAGPipeline` is a class that implements a Retrieval-Augmented Generation (RAG) pipeline with adalflow using documents. It has:\n",
    "\n",
    "- Efficient RAG Pipeline for handling large text files, embedding, and retrieval.\n",
    "- Supports token management and context truncation for LLM integration.\n",
    "- Generates accurate responses using retrieval-augmented generation (RAG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdalflowRAGPipeline:\n",
    "    def __init__(self,\n",
    "                 model_client=None,\n",
    "                 model_kwargs=None,\n",
    "                 embedding_model='all-MiniLM-L6-v2', \n",
    "                 vector_dim=384, \n",
    "                 top_k_retrieval=3,\n",
    "                 max_context_tokens=800):\n",
    "        \"\"\"\n",
    "        Initialize RAG Pipeline for handling large text files\n",
    "        \n",
    "        Args:\n",
    "            embedding_model (str): Sentence transformer model for embeddings\n",
    "            vector_dim (int): Dimension of embedding vectors\n",
    "            top_k_retrieval (int): Number of documents to retrieve\n",
    "            max_context_tokens (int): Maximum tokens to send to LLM\n",
    "        \"\"\"\n",
    "        # Initialize model client for generation\n",
    "        self.model_client = model_client\n",
    "        \n",
    "        # Initialize tokenizer for precise token counting\n",
    "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        \n",
    "        # Initialize embedding model\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        \n",
    "        # Initialize FAISS index for vector similarity search\n",
    "        self.index = IndexFlatL2(vector_dim)\n",
    "        \n",
    "        # Store document texts, embeddings, and metadata\n",
    "        self.documents = []\n",
    "        self.document_embeddings = []\n",
    "        self.document_metadata = []\n",
    "        \n",
    "        # Retrieval and context management parameters\n",
    "        self.top_k_retrieval = top_k_retrieval\n",
    "        self.max_context_tokens = max_context_tokens\n",
    "        \n",
    "        # Model generation parameters\n",
    "        self.model_kwargs = model_kwargs\n",
    "\n",
    "    def load_text_file(self, file_path: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Load a large text file and split into manageable chunks\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to the text file\n",
    "        \n",
    "        Returns:\n",
    "            List[str]: List of document chunks\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            # Read entire file\n",
    "            content = file.read()\n",
    "        \n",
    "        # Split content into chunks (e.g., 10 lines per chunk)\n",
    "        lines = content.split('\\n')\n",
    "        chunks = []\n",
    "        chunk_size = 10  # Adjust based on your file structure\n",
    "        \n",
    "        for i in range(0, len(lines), chunk_size):\n",
    "            chunk = '\\n'.join(lines[i:i+chunk_size])\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def add_documents_from_directory(self, directory_path: str):\n",
    "        \"\"\"\n",
    "        Add documents from all text files in a directory\n",
    "        \n",
    "        Args:\n",
    "            directory_path (str): Path to directory containing text files\n",
    "        \"\"\"\n",
    "        for filename in os.listdir(directory_path):\n",
    "            if filename.endswith('.txt'):\n",
    "                file_path = os.path.join(directory_path, filename)\n",
    "                document_chunks = self.load_text_file(file_path)\n",
    "                \n",
    "                for chunk in document_chunks:\n",
    "                    # Embed document chunk\n",
    "                    embedding = self.embedding_model.encode(chunk)\n",
    "                    \n",
    "                    # Add to index and document store\n",
    "                    self.index.add(np.array([embedding]))\n",
    "                    self.documents.append(chunk)\n",
    "                    self.document_embeddings.append(embedding)\n",
    "                    self.document_metadata.append({\n",
    "                        'filename': filename,\n",
    "                        'chunk_index': len(self.document_metadata)\n",
    "                    })\n",
    "\n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Count tokens in a given text\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text\n",
    "        \n",
    "        Returns:\n",
    "            int: Number of tokens\n",
    "        \"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "\n",
    "    def retrieve_and_truncate_context(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents and truncate to fit token limit\n",
    "        \n",
    "        Args:\n",
    "            query (str): Input query\n",
    "        \n",
    "        Returns:\n",
    "            str: Concatenated context within token limit\n",
    "        \"\"\"\n",
    "        # Retrieve relevant documents\n",
    "        query_embedding = self.embedding_model.encode(query)\n",
    "        distances, indices = self.index.search(\n",
    "            np.array([query_embedding]), \n",
    "            self.top_k_retrieval\n",
    "        )\n",
    "        \n",
    "        # Collect and truncate context\n",
    "        context = []\n",
    "        current_tokens = 0\n",
    "        \n",
    "        for idx in indices[0]:\n",
    "            doc = self.documents[idx]\n",
    "            doc_tokens = self.count_tokens(doc)\n",
    "            \n",
    "            # Check if adding this document would exceed token limit\n",
    "            if current_tokens + doc_tokens <= self.max_context_tokens:\n",
    "                context.append(doc)\n",
    "                current_tokens += doc_tokens\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        return \"\\n\\n\".join(context)\n",
    "\n",
    "    def generate_response(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response using retrieval-augmented generation\n",
    "        \n",
    "        Args:\n",
    "            query (str): User's input query\n",
    "        \n",
    "        Returns:\n",
    "            str: Generated response incorporating retrieved context\n",
    "        \"\"\"\n",
    "        # Retrieve and truncate context\n",
    "        retrieved_context = self.retrieve_and_truncate_context(query)\n",
    "        \n",
    "        # Construct context-aware prompt\n",
    "        full_prompt = f\"\"\"\n",
    "        Context Documents:\n",
    "        {retrieved_context}\n",
    "        \n",
    "        Query: {query}\n",
    "        \n",
    "        Generate a comprehensive response that:\n",
    "        1. Directly answers the query\n",
    "        2. Incorporates relevant information from the context documents\n",
    "        3. Provides clear and concise information\n",
    "        \"\"\"\n",
    "        \n",
    "        # Prepare API arguments\n",
    "        api_kwargs = self.model_client.convert_inputs_to_api_kwargs(\n",
    "            input=full_prompt,\n",
    "            model_kwargs=self.model_kwargs,\n",
    "            model_type=ModelType.LLM\n",
    "        )\n",
    "        \n",
    "        # Call API and parse response\n",
    "        response = self.model_client.call(\n",
    "            api_kwargs=api_kwargs, \n",
    "            model_type=ModelType.LLM\n",
    "        )\n",
    "        response_text = self.model_client.parse_chat_completion(response)\n",
    "        \n",
    "        return response_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`run_rag_pipeline` demonstrates how to use the AdalflowRAGPipeline to handle retrieval-augmented generation. It initializes the pipeline with specified retrieval and context token limits, loads documents from a directory, and processes a list of queries. For each query, the function retrieves relevant context, generates a response using the pipeline, and prints the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_pipeline(model_client, model_kwargs, documents, queries):\n",
    "\n",
    "    # Example usage of RAG pipeline\n",
    "    rag_pipeline = AdalflowRAGPipeline(\n",
    "        model_client=model_client,\n",
    "        model_kwargs=model_kwargs,\n",
    "        top_k_retrieval=1,  # Retrieve top 1 most relevant chunks\n",
    "        max_context_tokens=800  # Limit context to 1500 tokens\n",
    "    )\n",
    "\n",
    "    # Add documents from a directory of text files\n",
    "    rag_pipeline.add_documents_from_directory(documents)\n",
    "    \n",
    "    # Generate responses\n",
    "    for query in queries:\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        response = rag_pipeline.generate_response(query)\n",
    "        print(f\"Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What year was the Crystal Cavern discovered?\n",
      "Response: GeneratorOutput(id=None, data=None, error=None, usage=CompletionUsage(completion_tokens=14, prompt_tokens=203, total_tokens=217), raw_response='The Crystal Cavern was discovered in 1987 by divers.', metadata=None)\n",
      "\n",
      "Query: What is the name of the rare tree in Elmsworth?\n",
      "Response: GeneratorOutput(id=None, data=None, error=None, usage=CompletionUsage(completion_tokens=17, prompt_tokens=212, total_tokens=229), raw_response='The rare tree in Elmsworth is known as the \"Moonshade Willow\".', metadata=None)\n",
      "\n",
      "Query: What local legend claim that Lunaflits surrounds?\n",
      "Response: GeneratorOutput(id=None, data=None, error=None, usage=CompletionUsage(completion_tokens=19, prompt_tokens=206, total_tokens=225), raw_response='Local legend claims that Lunaflits are guardians of ancient treasure buried deep within the canyon.', metadata=None)\n",
      "\n",
      "Query: What year was the Crystal Cavern discovered?\n",
      "Response: GeneratorOutput(id=None, data=None, error=None, usage=CompletionUsage(completion_tokens=107, prompt_tokens=184, total_tokens=291), raw_response='The Crystal Cavern was discovered by divers in the year 1987 beneath the icy waters of Lake Aurora. The cavern is known for its shimmering quartz formations that refract sunlight into a spectrum of colors, and it is believed to have served as a sanctuary for an ancient civilization that revered the crystals as conduits to the spirit world. Artifacts recovered from the cavern are carved with intricate symbols, indicating a deep connection to celestial events. However, accessing the cavern is dangerous due to the freezing temperatures and strong currents of the lake.', metadata=None)\n",
      "\n",
      "Query: What is the name of the rare tree in Elmsworth?\n",
      "Response: GeneratorOutput(id=None, data=None, error=None, usage=CompletionUsage(completion_tokens=104, prompt_tokens=193, total_tokens=297), raw_response='The rare tree in Elmsworth is called the \"Moonshade Willow.\" It blooms once every seven years, emitting a soft glow from its blossoms. Villagers believe that meditating under its branches brings vivid dreams of the future. The tree\\'s bark contains a secret resin used in ancient healing rituals. Elders claim that the Moonshade Willow was a gift from a goddess to protect the village. Researchers have found that the tree can only thrive in Elmsworth\\'s unique soil, making it impossible to cultivate elsewhere.', metadata=None)\n",
      "\n",
      "Query: What local legend claim that Lunaflits surrounds?\n",
      "Response: GeneratorOutput(id=None, data=None, error=None, usage=CompletionUsage(completion_tokens=100, prompt_tokens=187, total_tokens=287), raw_response='Local legends claim that Lunaflits, the glowing insects found in the remote desert canyon, are believed to be guardians of ancient treasure buried deep within the canyon. These creatures emit a constant, soothing green light that illuminates the canyon at night, and their rhythmic light pulses form intricate patterns, suggesting a form of communication among them. The ethereal glow created by the Lunaflits and the rare moss reflecting their light have contributed to the mystical reputation of these insects as protectors of hidden riches.', metadata=None)\n"
     ]
    }
   ],
   "source": [
    "# setup_env()\n",
    "\n",
    "documents = '../../tutorials/assets/documents'\n",
    "\n",
    "queries = [\n",
    "    \"What year was the Crystal Cavern discovered?\",\n",
    "    \"What is the name of the rare tree in Elmsworth?\",\n",
    "    \"What local legend claim that Lunaflits surrounds?\"\n",
    "]\n",
    "\n",
    "groq_model_kwargs = {\n",
    "    \"model\": \"llama-3.2-1b-preview\",  # Use 16k model for larger context\n",
    "    \"temperature\": 0.1,\n",
    "    \"max_tokens\": 800,\n",
    "}\n",
    "\n",
    "openai_model_kwargs = {\n",
    "    \"model\": \"gpt-3.5-turbo\",\n",
    "    \"temperature\": 0.1,\n",
    "    \"max_tokens\": 800,\n",
    "}\n",
    "# Below example shows that adalflow can be used in a genric manner for any api provider\n",
    "# without worrying about prompt and parsing results\n",
    "run_rag_pipeline(GroqAPIClient(), groq_model_kwargs, documents, queries)\n",
    "run_rag_pipeline(OpenAIClient(), openai_model_kwargs, documents, queries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
